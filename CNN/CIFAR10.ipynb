{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PSGD_FASHION_MNIST.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Pn7E8tXofBdm"},"source":["#Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BaZdyOqH8ta1","executionInfo":{"status":"ok","timestamp":1616314029141,"user_tz":-300,"elapsed":4332,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"1a95aa68-eb3d-4142-8658-7f5338135423"},"source":["!pip install pkbar"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pkbar in /usr/local/lib/python3.7/dist-packages (0.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pkbar) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F0hq8_ykmK2J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616314031021,"user_tz":-300,"elapsed":6192,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"66ffbdeb-c032-4dbd-d530-e107a493d50b"},"source":["from google.colab import drive\n","from google.colab import files\n","import sys\n","import time\n","\n","drive.mount('/content/gdrive/', force_remount=True)\n","root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'Colab Notebooks/MS Thesis/PSGD Paper/'\n","results_dir = base_dir + 'CNN/results_fashion/'\n","logs_dir = base_dir + 'log'\n","sys.path.append(base_dir)\n","import preconditioned_stochastic_gradient_descent as psgd \n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aEZtit9cjlMG","executionInfo":{"status":"ok","timestamp":1616314031022,"user_tz":-300,"elapsed":6179,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["import matplotlib.pyplot as plt\n","import torch\n","from torch.autograd import grad\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import numpy as np\n","import time\n","import tqdm\n","import pkbar\n","import math\n","\n","from tabulate import tabulate\n","import scipy.io\n","from sklearn import metrics\n","import plotly.express as px\n","from torchsummary import summary\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKooB49HfKzR"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"roPf15eMfNuJ","executionInfo":{"status":"ok","timestamp":1616314031023,"user_tz":-300,"elapsed":6169,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["def plot_loss_metrics(xaxis,yaxis,title, x_label,y_label):\n"," \n","  fig = go.Figure()\n","  i = 0\n","  if(xaxis != None):\n","    for opt in opts:\n","      fig.add_trace(go.Scatter(x = xaxis[opt], y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","      i = i + 1\n","  else:\n","    for opt in opts:\n","      fig.add_trace(go.Scatter(y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","      i = i + 1\n","\n","  fig.update_layout(title=title, xaxis_title=x_label, yaxis_title=y_label, yaxis_type=\"log\")\n","  fig.show()\n","  fig.write_html(results_dir + title + \".html\")\n","\n","def plot_acc_metrics(xaxis,yaxis,title, x_label,y_label):\n"," \n","  fig = go.Figure()\n","  i = 0\n","  if(xaxis != None):\n","    for opt in opts:\n","      fig.add_trace(go.Scatter(x = xaxis[opt], y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","      i = i + 1\n","  else:\n","    for opt in opts:\n","      fig.add_trace(go.Scatter(y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","      i = i + 1\n","\n","  fig.update_layout(title=title, xaxis_title=x_label, yaxis_title=y_label, yaxis=dict(range=[0.75, 1]))\n","  fig.show()\n","  fig.write_html(results_dir + title + \".html\")\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ejKz-9nfUoy","executionInfo":{"status":"ok","timestamp":1616314031024,"user_tz":-300,"elapsed":6160,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["np.random.seed(0)\n","\n","# Parameter Settings\n","BATCH_SIZE = 64\n","test_BATCH_SIZE = 1000\n","EPOCHS = 20\n","GAP = 100"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t71vTNyumHao"},"source":["# Data Download"]},{"cell_type":"code","metadata":{"id":"LyioOEA_kdvB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616314031025,"user_tz":-300,"elapsed":6147,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"0ce8efdd-2bc1-4696-b8f4-d6130f179162"},"source":["train_loader = torch.utils.data.DataLoader(\n","        datasets.FashionMNIST('./data', train=True, download=True,           \n","                       transform=transforms.Compose([                       \n","                               transforms.ToTensor()])),    \n","                        batch_size=BATCH_SIZE, shuffle=True, num_workers = 4, pin_memory = True)\n","test_loader = torch.utils.data.DataLoader(    \n","        datasets.FashionMNIST('./data', train=False, transform=transforms.Compose([\n","                       transforms.ToTensor()])),    \n","                        batch_size=test_BATCH_SIZE, shuffle=True, num_workers=4, pin_memory = True)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LAMhwYaRmbIF","executionInfo":{"status":"ok","timestamp":1616314031027,"user_tz":-300,"elapsed":6133,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["n_batches = np.ceil(len(train_loader.dataset)/BATCH_SIZE)\n","n_test_batches = np.ceil(len(test_loader.dataset)/test_BATCH_SIZE)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAJdx1Ga6xFX","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1616314031028,"user_tz":-300,"elapsed":6119,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"e315dd1d-365f-4a77-e57b-7eb308ac4a75"},"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Running on the GPU\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla T4'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"O55VB__slpJx"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"2N_7RpDVlbZE","executionInfo":{"status":"ok","timestamp":1616314031029,"user_tz":-300,"elapsed":6104,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["\"\"\"input image size for the original LeNet5 is 32x32, here is 28x28\"\"\"\n","torch.manual_seed(1)\n","torch.cuda.manual_seed_all(1)\n","def initialize_weights():\n","    W1 = torch.nn.init.xavier_uniform_((0.1*torch.randn(1*5*5+1,  6)).clone().detach().requires_grad_(True)).to(device)\n","    W2 = torch.nn.init.xavier_uniform_((0.1*torch.randn(6*5*5+1,  16)).clone().detach().requires_grad_(True)).to(device)\n","    W3 = torch.nn.init.xavier_uniform_((0.1*torch.randn(16*4*4+1, 120)).clone().detach().requires_grad_(True)).to(device)\n","    W4 = torch.nn.init.xavier_uniform_((0.1*torch.randn(120+1,    84)).clone().detach().requires_grad_(True)).to(device)\n","    W5 = torch.nn.init.xavier_uniform_((0.1*torch.randn(84+1,     10)).clone().detach().requires_grad_(True)).to(device)\n","    Ws = [W1, W2, W3, W4, W5]\n","    return Ws\n","\n","def LeNet5(x, return_all = False): \n","    W1, W2, W3, W4, W5 = Ws\n","    x1 = F.relu(F.conv2d(x, W1[:-1].view(6,1,5,5), bias=W1[-1]))\n","    x2 = F.max_pool2d(x1, 2)\n","    x3 = F.relu(F.conv2d(x2, W2[:-1].view(16,6,5,5), bias=W2[-1]))\n","    x4 = F.max_pool2d(x3, 2)\n","    x5 = F.relu(x4.view(-1, 16*4*4).mm(W3[:-1]) + W3[-1])\n","    x6 = F.relu(x5.mm(W4[:-1]) + W4[-1])\n","    # x = F.dropout(x, 0.3, training = True)\n","    y = x6.mm(W5[:-1]) + W5[-1]\n","    if return_all:\n","      return F.log_softmax(y, dim = 1), [x, x1, x2, x3, x4, x5, x6, y]\n","    return F.log_softmax(y, dim=1)\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hOFi1DHlx2Q"},"source":["# Loss Function"]},{"cell_type":"code","metadata":{"id":"ttaD9mDElrR1","executionInfo":{"status":"ok","timestamp":1616314031030,"user_tz":-300,"elapsed":6095,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["def train_loss(data, target):\n","    y = LeNet5(data)\n","    loss = F.nll_loss(y, target)\n","    # loss = F.cross_entropy(y, target)\n","    _, max_indices = torch.max(y, dim = 1)\n","    accuracy = (max_indices == target).sum(dtype=torch.float32)/max_indices.size(0)\n","    return loss, accuracy\n","\n","def test_loss():\n","    loss = 0\n","    accuracy = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            y = LeNet5(data)\n","            loss += F.nll_loss(y, target)\n","            _, pred = torch.max(y, dim=1)\n","            accuracy += (pred == target).sum(dtype=torch.float32)/pred.size(0)\n","    return loss.item()/n_test_batches, accuracy.item()/n_test_batches\n","\n","def save_start_condition(trainlosslist, testlosslist,trainacclist, testacclist, timelist):\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      data, target = data.to(device), target.to(device)\n","      loss, accuracy = train_loss(data, target)\n","      trainloss += loss\n","      trainacc += accuracy\n","      \n","  \n","    timelist.append(0)\n","\n","    testloss, testacc = test_loss()\n","\n","    trainlosslist.append(trainloss.item()/n_batches)\n","    trainacclist.append(trainacc.item()/n_batches)\n","    testlosslist.append(testloss)\n","    testacclist.append(testacc)\n","    print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    .format(0, trainlosslist[-1], testlosslist[-1], trainacclist[-1], testacclist[-1],np.sum(timelist)))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5nub-2OIB1v"},"source":["# ALL TEST"]},{"cell_type":"markdown","metadata":{"id":"KHHRjmBv5nGy"},"source":["## SGD"]},{"cell_type":"code","metadata":{"id":"6-Kz6zZw5osv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616314187377,"user_tz":-300,"elapsed":162427,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"3e4efe2d-7f9f-4013-d79a-6e26e79bd36d"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    n = 0\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        trainacc += accuracy\n","        \n","        with torch.no_grad():\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*grads[i]\n","\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","    \n","    testloss, testacc = test_loss()\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","\n","    step_size = 0.01**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'sgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4306401169376333; test loss: 2.4298145294189455, train_accuracy: 0.10022987739872068, test_accuracy:0.1004000186920166, time: 0\n","Epoch: 1/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.5971 - acc: 0.7780 - val_loss: 0.5506 - val_acc: 0.7897\n","Epoch: 2/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.3921 - acc: 0.8547 - val_loss: 0.3851 - val_acc: 0.8612\n","Epoch: 3/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.3452 - acc: 0.8726 - val_loss: 0.3752 - val_acc: 0.8603\n","Epoch: 4/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.3220 - acc: 0.8798 - val_loss: 0.3578 - val_acc: 0.8669\n","Epoch: 5/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.3072 - acc: 0.8854 - val_loss: 0.3573 - val_acc: 0.8691\n","Epoch: 6/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2980 - acc: 0.8897 - val_loss: 0.3378 - val_acc: 0.8751\n","Epoch: 7/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2926 - acc: 0.8913 - val_loss: 0.3354 - val_acc: 0.8778\n","Epoch: 8/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2889 - acc: 0.8937 - val_loss: 0.3342 - val_acc: 0.8790\n","Epoch: 9/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2868 - acc: 0.8941 - val_loss: 0.3330 - val_acc: 0.8782\n","Epoch: 10/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2855 - acc: 0.8949 - val_loss: 0.3327 - val_acc: 0.8793\n","Epoch: 11/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2845 - acc: 0.8953 - val_loss: 0.3322 - val_acc: 0.8793\n","Epoch: 12/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2840 - acc: 0.8956 - val_loss: 0.3321 - val_acc: 0.8799\n","Epoch: 13/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2839 - acc: 0.8956 - val_loss: 0.3320 - val_acc: 0.8798\n","Epoch: 14/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2836 - acc: 0.8955 - val_loss: 0.3321 - val_acc: 0.8801\n","Epoch: 15/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2834 - acc: 0.8956 - val_loss: 0.3320 - val_acc: 0.8800\n","Epoch: 16/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2831 - acc: 0.8958 - val_loss: 0.3320 - val_acc: 0.8796\n","Epoch: 17/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2834 - acc: 0.8956 - val_loss: 0.3320 - val_acc: 0.8798\n","Epoch: 18/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2833 - acc: 0.8956 - val_loss: 0.3320 - val_acc: 0.8797\n","Epoch: 19/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2834 - acc: 0.8956 - val_loss: 0.3320 - val_acc: 0.8797\n","Epoch: 20/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2834 - acc: 0.8955 - val_loss: 0.3320 - val_acc: 0.8797\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dYV3Ro1nl9-w"},"source":["## Adam"]},{"cell_type":"code","metadata":{"id":"ShkQySPkl0jM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90d78629-1bea-4a94-f06a-d02021c7124a"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","\n","m0 = [torch.zeros(W.shape).to(device) for W in Ws]\n","v0 = [torch.zeros(W.shape).to(device) for W in Ws]\n","step_size = 0.005\n","cnt = 0\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","for epoch in range(EPOCHS):\n","    n = 0\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws)#, create_graph=True)\n","        trainloss += loss\n","        trainacc += accuracy\n","        \n","\n","        with torch.no_grad():\n","            lmbd = min(cnt/(cnt+1), 0.9)\n","            m0 = [lmbd*old + (1.0-lmbd)*new for (old, new) in zip(m0, grads)]\n","            lmbd = min(cnt/(cnt+1), 0.999)\n","            v0 = [lmbd*old + (1.0-lmbd)*new*new for (old, new) in zip(v0, grads)]\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_size*(m0[i]/torch.sqrt(v0[i] + 1e-8))\n","            cnt = cnt + 1\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1    \n","        \n","    t1 = time.time() - t0\n","    times.append(t1)\n","\n","    testloss, testacc = test_loss()\n","\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    step_size = 0.01**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    # .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'adam.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4306401169376333; test loss: 2.4298145294189455, train_accuracy: 0.10022987739872068, test_accuracy:0.1004000186920166, time: 0\n","Epoch: 1/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.4894 - acc: 0.8187 - val_loss: 0.3975 - val_acc: 0.8534\n","Epoch: 2/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.3286 - acc: 0.8771 - val_loss: 0.3450 - val_acc: 0.8756\n","Epoch: 3/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2793 - acc: 0.8960 - val_loss: 0.3220 - val_acc: 0.8859\n","Epoch: 4/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.2475 - acc: 0.9073 - val_loss: 0.3094 - val_acc: 0.8860\n","Epoch: 5/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2262 - acc: 0.9146 - val_loss: 0.3048 - val_acc: 0.8903\n","Epoch: 6/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.2106 - acc: 0.9210 - val_loss: 0.2934 - val_acc: 0.8943\n","Epoch: 7/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.2004 - acc: 0.9248 - val_loss: 0.2903 - val_acc: 0.8964\n","Epoch: 8/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.1937 - acc: 0.9271 - val_loss: 0.2932 - val_acc: 0.8974\n","Epoch: 9/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.1897 - acc: 0.9290 - val_loss: 0.2923 - val_acc: 0.8982\n","Epoch: 10/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.1871 - acc: 0.9296 - val_loss: 0.2926 - val_acc: 0.8986\n","Epoch: 11/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.1855 - acc: 0.9308 - val_loss: 0.2920 - val_acc: 0.8992\n","Epoch: 12/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.1846 - acc: 0.9308 - val_loss: 0.2922 - val_acc: 0.8987\n","Epoch: 13/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.1841 - acc: 0.9308 - val_loss: 0.2924 - val_acc: 0.8988\n","Epoch: 14/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.1835 - acc: 0.9311 - val_loss: 0.2923 - val_acc: 0.8983\n","Epoch: 15/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.1834 - acc: 0.9312 - val_loss: 0.2923 - val_acc: 0.8987\n","Epoch: 16/20\n","938/938 [==============================] - 7s 8ms/step - loss: 0.1830 - acc: 0.9311 - val_loss: 0.2923 - val_acc: 0.8984\n","Epoch: 17/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.1832 - acc: 0.9312 - val_loss: 0.2923 - val_acc: 0.8984\n","Epoch: 18/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.1832 - acc: 0.9312 - val_loss: 0.2923 - val_acc: 0.8983\n","Epoch: 19/20\n","938/938 [==============================] - 8s 8ms/step - loss: 0.1832 - acc: 0.9312 - val_loss: 0.2923 - val_acc: 0.8985\n","Epoch: 20/20\n","867/938 [==========================>...] - ETA: 0s - loss: 0.1833 - acc: 0.9312"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8AoYYDPCIMkn"},"source":["## Full Kronecker"]},{"cell_type":"code","metadata":{"id":"lRcCtC_omg8l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616314685424,"user_tz":-300,"elapsed":660438,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"c011dac5-844a-4d11-c16c-f8d2979517d0"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    n = 0\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        trainacc += accuracy\n","\n","        v = [torch.randn(W.shape).to(device) for W in Ws]\n","        Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            pre_grads = [psgd.precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","    \n","    testloss, testacc = test_loss()\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    step_size = 0.01**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'Kron.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":13,"outputs":[{"output_type":"stream","text":["938/938 [==============================] - 17s 18ms/step - loss: 0.4761 - acc: 0.8255 - val_loss: 0.4210 - val_acc: 0.8399\n","Epoch: 2/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.2988 - acc: 0.8892 - val_loss: 0.3366 - val_acc: 0.8789\n","Epoch: 3/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.2313 - acc: 0.9136 - val_loss: 0.3072 - val_acc: 0.8913\n","Epoch: 4/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1859 - acc: 0.9308 - val_loss: 0.3041 - val_acc: 0.8979\n","Epoch: 5/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1564 - acc: 0.9427 - val_loss: 0.3005 - val_acc: 0.9020\n","Epoch: 6/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1389 - acc: 0.9496 - val_loss: 0.3074 - val_acc: 0.9018\n","Epoch: 7/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1284 - acc: 0.9538 - val_loss: 0.3106 - val_acc: 0.9015\n","Epoch: 8/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1225 - acc: 0.9562 - val_loss: 0.3138 - val_acc: 0.9019\n","Epoch: 9/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1188 - acc: 0.9574 - val_loss: 0.3153 - val_acc: 0.9019\n","Epoch: 10/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1168 - acc: 0.9580 - val_loss: 0.3158 - val_acc: 0.9013\n","Epoch: 11/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1154 - acc: 0.9588 - val_loss: 0.3167 - val_acc: 0.9016\n","Epoch: 12/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1146 - acc: 0.9592 - val_loss: 0.3172 - val_acc: 0.9015\n","Epoch: 13/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1142 - acc: 0.9594 - val_loss: 0.3174 - val_acc: 0.9016\n","Epoch: 14/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1140 - acc: 0.9593 - val_loss: 0.3176 - val_acc: 0.9018\n","Epoch: 15/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1137 - acc: 0.9595 - val_loss: 0.3176 - val_acc: 0.9016\n","Epoch: 16/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1138 - acc: 0.9595 - val_loss: 0.3177 - val_acc: 0.9016\n","Epoch: 17/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1136 - acc: 0.9597 - val_loss: 0.3177 - val_acc: 0.9016\n","Epoch: 18/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1136 - acc: 0.9596 - val_loss: 0.3177 - val_acc: 0.9016\n","Epoch: 19/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1134 - acc: 0.9597 - val_loss: 0.3178 - val_acc: 0.9016\n","Epoch: 20/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1135 - acc: 0.9596 - val_loss: 0.3178 - val_acc: 0.9016\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3XDQPC9p5vgk"},"source":["## DPSGD Approach 1"]},{"cell_type":"code","metadata":{"id":"m7A2qd5XHXzL","executionInfo":{"status":"ok","timestamp":1616314685427,"user_tz":-300,"elapsed":660426,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["_tiny = 1.2e-38 \n"," # pi = (torch.trace(Ql)*Qr.shape[0])/(torch.trace(Qr)*Ql.shape[0])\n","    # \n","\n","    \n","def precond_grad_kron(Ql, Qr, Grad):\n","    P1 = Ql.t().mm(Ql)\n","    P2 = Qr.t().mm(Qr)\n","    pi = (torch.trace(P1)*P2.shape[0])/(torch.trace(P2)*P1.shape[0])\n","    IL = torch.ones(P1.shape[0]).to(device)\n","    IR = (torch.ones(P2.shape[0])).to(device)\n","    P1 = P1 + torch.diag(torch.sqrt((pi)*(eta + lambd))*IL)\n","    P2 = P2 + torch.diag(torch.sqrt((1/pi)*(eta + lambd))*IR)\n","\n","    return P1.mm(Grad).mm(P2)\n","\n","def update_lambda(loss1, loss2, M, lambd, omega):\n","    \n","    r = abs(loss2 - loss1)/(M)\n","    # print(r, M, lambd)\n","    if r > 3/4:\n","      lambd = lambd*omega\n","    elif r < 1/4:\n","      lambd = lambd / omega\n","    return lambd\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"L07W8H555y9S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616315057418,"user_tz":-300,"elapsed":1032402,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"0cae9cd5-2bb1-492c-caef-ee44fa07460a"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","lambd = 1\n","update_after = 5\n","omega = (19/20)**update_after\n","\n","eta = 1e-5\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    n = 0\n","    t0 = time.time()\n","    \n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        trainacc += accuracy\n","        if n % 1 == 0:\n","          v = [torch.randn(W.shape).to(device) for W in Ws]\n","          Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            pre_grads = [precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","            if n % update_after == 0 and lambd > 1e-10:\n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(g*pg) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(-step_size * pg*psgd.precond_grad_kron(q[0], q[1], -step_size * pg)) \\\n","                #              for (g, pg, q) in zip(grads, pre_grads, Qs)])\n","                loss2 = F.nll_loss(LeNet5(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M,  lambd, omega)\n","\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","\n","    testloss, testacc = test_loss()\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    step_size = 0.01**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'Kron_damped.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4306401169376333; test loss: 2.4298145294189455, train_accuracy: 0.10022987739872068, test_accuracy:0.1004000186920166, time: 0\n","Epoch: 1/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.4835 - acc: 0.8203 - val_loss: 0.4164 - val_acc: 0.8450\n","Epoch: 2/20\n","938/938 [==============================] - 18s 20ms/step - loss: 0.2795 - acc: 0.8941 - val_loss: 0.3237 - val_acc: 0.8801\n","Epoch: 3/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.2136 - acc: 0.9191 - val_loss: 0.2892 - val_acc: 0.8930\n","Epoch: 4/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.1676 - acc: 0.9369 - val_loss: 0.2983 - val_acc: 0.8941\n","Epoch: 5/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.1385 - acc: 0.9492 - val_loss: 0.2949 - val_acc: 0.9006\n","Epoch: 6/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.1210 - acc: 0.9560 - val_loss: 0.3004 - val_acc: 0.9017\n","Epoch: 7/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.1101 - acc: 0.9606 - val_loss: 0.3083 - val_acc: 0.9022\n","Epoch: 8/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.1038 - acc: 0.9638 - val_loss: 0.3120 - val_acc: 0.9024\n","Epoch: 9/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.1002 - acc: 0.9654 - val_loss: 0.3142 - val_acc: 0.9015\n","Epoch: 10/20\n","938/938 [==============================] - 18s 20ms/step - loss: 0.0980 - acc: 0.9663 - val_loss: 0.3152 - val_acc: 0.9016\n","Epoch: 11/20\n","938/938 [==============================] - 18s 20ms/step - loss: 0.0966 - acc: 0.9667 - val_loss: 0.3164 - val_acc: 0.9020\n","Epoch: 12/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0959 - acc: 0.9669 - val_loss: 0.3169 - val_acc: 0.9016\n","Epoch: 13/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0954 - acc: 0.9671 - val_loss: 0.3173 - val_acc: 0.9017\n","Epoch: 14/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0952 - acc: 0.9673 - val_loss: 0.3175 - val_acc: 0.9020\n","Epoch: 15/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0950 - acc: 0.9672 - val_loss: 0.3176 - val_acc: 0.9020\n","Epoch: 16/20\n","938/938 [==============================] - 18s 20ms/step - loss: 0.0950 - acc: 0.9673 - val_loss: 0.3177 - val_acc: 0.9019\n","Epoch: 17/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0948 - acc: 0.9674 - val_loss: 0.3177 - val_acc: 0.9020\n","Epoch: 18/20\n","938/938 [==============================] - 18s 20ms/step - loss: 0.0947 - acc: 0.9674 - val_loss: 0.3178 - val_acc: 0.9021\n","Epoch: 19/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0947 - acc: 0.9675 - val_loss: 0.3178 - val_acc: 0.9021\n","Epoch: 20/20\n","938/938 [==============================] - 18s 19ms/step - loss: 0.0947 - acc: 0.9675 - val_loss: 0.3178 - val_acc: 0.9021\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3oq1QQCPEdZh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616315065584,"user_tz":-300,"elapsed":1040552,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"203dafd7-f3bd-4215-874f-e786fcd1c793"},"source":["full_data, full_target = [],[]\n","for batch_idx, (data, target) in enumerate(train_loader):\n","      data, target = data, target\n","      full_data.extend(data)\n","      full_target.extend(data)\n","full_data = torch.stack(full_data).to(device)\n","full_target = torch.stack(full_target)#.to(device)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-qIgKzzS1i7w","executionInfo":{"status":"aborted","timestamp":1616315065586,"user_tz":-300,"elapsed":1040532,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["# torch.manual_seed(1)\n","# Ws = initialize_weights()\n","# Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","# dQs = [[torch.ones(W.shape[0],1).to(device), torch.ones(1,W.shape[1]).to(device)] for W in Ws]\n","\n","# step_size = 0.1\n","# grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","# TrainLoss, TestLoss = [], []\n","# TrainAcc, TestAcc = [], []\n","# times = []\n","# save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","# full_pre_grads = [torch.zeros(W.shape).to(device) for W in Ws]\n","# lambd = 1\n","# update_after = 5\n","# omega = (19/20)**update_after\n","\n","# eta = 1e-5\n","# for epoch in range(EPOCHS):\n","#     kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","#     trainloss = 0.0\n","#     trainacc = 0.0\n","#     n = 0\n","#     t0 = time.time()\n","#     loss, accuracy = train_loss(full_data, fulltarget)\n","#     grads = grad(trainloss, Ws, create_graph=True)\n","#     v = [torch.randn(W.shape).to(device) for W in Ws]\n","#     Hv = grad(grads, Ws, v)\n","#     with torch.no_grad():\n","#         Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","#         full_pre_grads = [psgd.precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","      \n","\n","    \n","#     for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","#         data, target = data.to(device), target.to(device)\n","#         loss, accuracy = train_loss(data, target)\n","        \n","#         grads = grad(loss, Ws, create_graph=True)\n","        \n","#         trainloss += loss\n","#         trainacc += accuracy          \n","        \n","#         with torch.no_grad():\n","#             Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","#             pre_grads = [psgd.precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","#             damp_grads = [((lambd+eta)**0.5)*g for g in grads]\n","#             pre_grads = [pg+dg for (pg, dg) in zip(pre_grads, damp_grads)] \n","#             grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","#             step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","#             for i in range(len(Ws)):\n","#                 Ws[i] -= step_adjust*step_size*(pre_grads[i] - full_pre_grads[i])\n","\n","#             if n % update_after == 0 and lambd > 1e-10:\n","#                 M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","#                 # M = 0.5*sum([torch.sum(g*pg) for (g, pg) in zip(grads, pre_grads)])\n","#                 # M = 0.5*sum([torch.sum(-step_size * pg*psgd.precond_grad_kron(q[0], q[1], -step_size * pg)) \\\n","#                 #              for (g, pg, q) in zip(grads, pre_grads, Qs)])\n","#                 loss2 = F.nll_loss(LeNet5(data), target)\n","#                 loss1 = loss\n","#                 lambd = update_lambda(loss1, loss2, M,  lambd, omega)\n","\n","#         kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","#         n += 1\n","\n","#     t1 = time.time() - t0\n","#     times.append(t1)\n","#     TrainLoss.append(trainloss.item()/n_batches)\n","#     TrainAcc.append(trainacc.item()/n_batches)\n","\n","#     testloss, testacc = test_loss()\n","\n","#     TestLoss.append(testloss)\n","#     TestAcc.append(testacc)\n","#     kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","#     step_size = 0.01**(1/9)*step_size\n","#     # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","#     #  .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","# # scipy.io.savemat(results_dir + 'Kron_damped.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qzn9JO8JGH7m"},"source":["## DPSGD Approach 2"]},{"cell_type":"code","metadata":{"id":"l5ivNBPX8_Qv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616315644786,"user_tz":-300,"elapsed":346239,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"f1aef110-9411-45cb-d416-fcf93b733a04"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","dQs = [[torch.ones(W.shape[0],1).to(device), torch.ones(1,W.shape[1]).to(device)] for W in Ws]\n","\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","lambd = 1\n","update_after = 5\n","omega = (19/20)**update_after\n","\n","eta = 1e-5\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    n = 0\n","    t0 = time.time()\n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        trainacc += accuracy\n","        if n % 1 == 0:\n","          v = [torch.randn(W.shape).to(device) for W in Ws]\n","          Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            pre_grads = [psgd.precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","            damp_grads = [((lambd+eta)**0.5)*g for g in grads]\n","            pre_grads = [pg+dg for (pg, dg) in zip(pre_grads, damp_grads)] \n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","            if n % update_after == 0 and lambd > 1e-10:\n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(g*pg) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(-step_size * pg*psgd.precond_grad_kron(q[0], q[1], -step_size * pg)) \\\n","                #              for (g, pg, q) in zip(grads, pre_grads, Qs)])\n","                loss2 = F.nll_loss(LeNet5(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M,  lambd, omega)\n","\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","\n","    testloss, testacc = test_loss()\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    step_size = 0.01**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'Kron_damped2.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4306401169376333; test loss: 2.4298145294189455, train_accuracy: 0.10022987739872068, test_accuracy:0.1004000186920166, time: 0\n","Epoch: 1/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.4691 - acc: 0.8291 - val_loss: 0.3893 - val_acc: 0.8508\n","Epoch: 2/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.2822 - acc: 0.8955 - val_loss: 0.3207 - val_acc: 0.8844\n","Epoch: 3/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.2137 - acc: 0.9204 - val_loss: 0.2894 - val_acc: 0.8955\n","Epoch: 4/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1665 - acc: 0.9382 - val_loss: 0.2848 - val_acc: 0.9019\n","Epoch: 5/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1367 - acc: 0.9498 - val_loss: 0.2906 - val_acc: 0.9056\n","Epoch: 6/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1195 - acc: 0.9571 - val_loss: 0.2969 - val_acc: 0.9063\n","Epoch: 7/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1087 - acc: 0.9621 - val_loss: 0.3031 - val_acc: 0.9059\n","Epoch: 8/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.1028 - acc: 0.9640 - val_loss: 0.3069 - val_acc: 0.9054\n","Epoch: 9/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0992 - acc: 0.9653 - val_loss: 0.3099 - val_acc: 0.9038\n","Epoch: 10/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0972 - acc: 0.9661 - val_loss: 0.3107 - val_acc: 0.9032\n","Epoch: 11/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0957 - acc: 0.9669 - val_loss: 0.3120 - val_acc: 0.9038\n","Epoch: 12/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0950 - acc: 0.9671 - val_loss: 0.3126 - val_acc: 0.9034\n","Epoch: 13/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0946 - acc: 0.9675 - val_loss: 0.3130 - val_acc: 0.9036\n","Epoch: 14/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0943 - acc: 0.9676 - val_loss: 0.3132 - val_acc: 0.9034\n","Epoch: 15/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0941 - acc: 0.9677 - val_loss: 0.3133 - val_acc: 0.9034\n","Epoch: 16/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0941 - acc: 0.9676 - val_loss: 0.3134 - val_acc: 0.9034\n","Epoch: 17/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0939 - acc: 0.9677 - val_loss: 0.3135 - val_acc: 0.9034\n","Epoch: 18/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0939 - acc: 0.9677 - val_loss: 0.3135 - val_acc: 0.9034\n","Epoch: 19/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0938 - acc: 0.9677 - val_loss: 0.3135 - val_acc: 0.9034\n","Epoch: 20/20\n","938/938 [==============================] - 17s 18ms/step - loss: 0.0939 - acc: 0.9677 - val_loss: 0.3135 - val_acc: 0.9034\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pzF0kQ3g7Bb0"},"source":["## D-PDSGD w/ momentum"]},{"cell_type":"code","metadata":{"id":"OM3_pW06XRPM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616317666987,"user_tz":-300,"elapsed":380803,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"6b69fe0a-bdc1-4527-9bc8-cf5f457bd571"},"source":["def precond_kron(Ql, Qr, Pl, Pr, beta):\n","    P1 = Ql.t().mm(Ql)\n","    P2 = Qr.t().mm(Qr)\n","    pi = (torch.trace(P1)*P2.shape[0])/(torch.trace(P2)*P1.shape[0])\n","    IL = torch.ones(P1.shape[0]).to(device)\n","    IR = (torch.ones(P2.shape[0])).to(device)\n","    P1 = P1 + torch.diag(torch.sqrt((pi)*(eta + lambd))*IL)\n","    P2 = P2 + torch.diag(torch.sqrt((1/pi)*(eta + lambd))*IR)\n","\n","    Pl = beta*Pl + (1-beta)*P1 \n","    Pr = beta*Pr + (1-beta)*P2 \n","\n","    return [P1, P2, Pl, Pr]\n","\n","def precond_kron2(Ql, Qr, Pl, Pr, beta):\n","    P1 = Ql.t().mm(Ql)\n","    P2 = Qr.t().mm(Qr)\n","    Pl = beta*Pl + (1-beta)*P1 \n","    Pr = beta*Pr + (1-beta)*P2 \n","    return [P1, P2, Pl, Pr]\n","\n","def precond_grad_kron2(Pl, Pr, Grad):\n","    return Pl.mm(Grad).mm(Pr)\n","\n","torch.manual_seed(1)\n","Ws = initialize_weights()\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","Ps = [[torch.zeros(W.shape[0]).to(device), torch.zeros(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","lambd = 1\n","update_after = 5\n","omega = (19/20)**update_after\n","eta = 1e-5\n","beta = 0.7\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    n = 0\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    t0 = time.time()\n","   \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        trainacc += accuracy\n","\n","        v = [torch.randn(W.shape).to(device) for W in Ws]\n","        Hv = grad(grads, Ws, v)\n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            beta = min(n/(n+1), 0.7)\n","            Ps = [precond_kron(q[0], q[1], p[0], p[1], beta) for (q, p) in zip(Qs, Ps)]\n","            pre_grads = [precond_grad_kron2(p[2], p[3], g) for (p, g) in zip(Ps, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","            if n % update_after == 0 and lambd > 1e-10:\n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                loss2 = F.nll_loss(LeNet5(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M, lambd, omega)\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","\n","    testloss, testacc = test_loss()\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    step_size = 0.01**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'mod_psgd1.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4306401169376333; test loss: 2.4298145294189455, train_accuracy: 0.10022987739872068, test_accuracy:0.1004000186920166, time: 0\n","Epoch: 1/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.4690 - acc: 0.8265 - val_loss: 0.4447 - val_acc: 0.8379\n","Epoch: 2/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.2772 - acc: 0.8948 - val_loss: 0.3203 - val_acc: 0.8803\n","Epoch: 3/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.2097 - acc: 0.9212 - val_loss: 0.2901 - val_acc: 0.8925\n","Epoch: 4/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.1631 - acc: 0.9391 - val_loss: 0.2892 - val_acc: 0.8998\n","Epoch: 5/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.1332 - acc: 0.9507 - val_loss: 0.3007 - val_acc: 0.9000\n","Epoch: 6/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.1154 - acc: 0.9580 - val_loss: 0.3075 - val_acc: 0.9005\n","Epoch: 7/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.1047 - acc: 0.9630 - val_loss: 0.3143 - val_acc: 0.9008\n","Epoch: 8/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0985 - acc: 0.9652 - val_loss: 0.3181 - val_acc: 0.9000\n","Epoch: 9/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0949 - acc: 0.9670 - val_loss: 0.3211 - val_acc: 0.9007\n","Epoch: 10/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0928 - acc: 0.9675 - val_loss: 0.3219 - val_acc: 0.9004\n","Epoch: 11/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0915 - acc: 0.9681 - val_loss: 0.3231 - val_acc: 0.9010\n","Epoch: 12/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0907 - acc: 0.9684 - val_loss: 0.3239 - val_acc: 0.9012\n","Epoch: 13/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0903 - acc: 0.9687 - val_loss: 0.3241 - val_acc: 0.9012\n","Epoch: 14/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0900 - acc: 0.9689 - val_loss: 0.3243 - val_acc: 0.9012\n","Epoch: 15/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0898 - acc: 0.9690 - val_loss: 0.3245 - val_acc: 0.9010\n","Epoch: 16/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0898 - acc: 0.9690 - val_loss: 0.3245 - val_acc: 0.9010\n","Epoch: 17/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0896 - acc: 0.9691 - val_loss: 0.3246 - val_acc: 0.9010\n","Epoch: 18/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0896 - acc: 0.9691 - val_loss: 0.3246 - val_acc: 0.9010\n","Epoch: 19/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0895 - acc: 0.9692 - val_loss: 0.3246 - val_acc: 0.9010\n","Epoch: 20/20\n","938/938 [==============================] - 19s 20ms/step - loss: 0.0896 - acc: 0.9691 - val_loss: 0.3246 - val_acc: 0.9010\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yj22SVg7tVwA"},"source":["##KFAC"]},{"cell_type":"code","metadata":{"id":"gNvGRjQ5tdt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616318112504,"user_tz":-300,"elapsed":256095,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"56fe4bb5-204f-4808-c770-8d57dca06d72"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","from kfac import KFAC\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class LeNet5_K(nn.Module):\n","    def __init__(self):\n","        super(LeNet5_K, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n","        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n","        self.fc1 = nn.Linear(256, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(-1, 256)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return F.log_softmax(x, dim=1)\n","\n","def test_loss_K(model):\n","    model.eval()\n","    loss = 0\n","    accuracy = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            y = model(data)\n","            loss += F.nll_loss(y, target)\n","            _, pred = torch.max(y, dim=1)\n","            accuracy += (pred == target).sum(dtype=torch.float32)/pred.size(0)\n","    return loss.item()/n_test_batches, accuracy/n_test_batches\n","\n","model = LeNet5_K().to(device)\n","preconditioner = KFAC(model, 0.001, alpha=0.05, sua = True)\n","lr0 = 0.01\n","optimizer = optim.SGD(model.parameters(), lr=lr0)\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    n = 0\n","    model.train()\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        \n","        loss = F.nll_loss(output, target)\n","        _, max_ind = torch.max(output, dim = 1)\n","        accuracy = (max_ind == target).sum(dtype=torch.float32)/max_ind.size(0) \n","\n","        trainloss += loss\n","        trainacc += accuracy\n","\n","        loss.backward()\n","        preconditioner.step()\n","        optimizer.step()\n","\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","        \n","    t1 = time.time() - t0\n","    times.append(t1)\n","\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","    \n","    \n","    lr0 = 0.01**(1/9)*lr0\n","    optimizer.param_groups[0]['lr'] = lr0\n","    testloss, testacc = test_loss_K(model)\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'KFAC.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4305425127432034; test loss: 2.429814910888672, train_accuracy: 0.10026319296375266, test_accuracy:0.10040000677108765, time: 0\n","Epoch: 1/20\n","  0/938 [..............................] - ETA: 0s - loss: 0.0000e+00 - acc: 0.0000e+00"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning:\n","\n","Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["938/938 [==============================] - 12s 13ms/step - loss: 0.4162 - acc: 0.8475 - val_loss: 0.3363 - val_acc: 0.8751\n","Epoch: 2/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.2718 - acc: 0.9002 - val_loss: 0.3077 - val_acc: 0.8842\n","Epoch: 3/20\n","938/938 [==============================] - 13s 14ms/step - loss: 0.2204 - acc: 0.9175 - val_loss: 0.2923 - val_acc: 0.8936\n","Epoch: 4/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1865 - acc: 0.9321 - val_loss: 0.2926 - val_acc: 0.8971\n","Epoch: 5/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1648 - acc: 0.9401 - val_loss: 0.2938 - val_acc: 0.8959\n","Epoch: 6/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1510 - acc: 0.9462 - val_loss: 0.2967 - val_acc: 0.8976\n","Epoch: 7/20\n","938/938 [==============================] - 13s 13ms/step - loss: 0.1420 - acc: 0.9502 - val_loss: 0.2994 - val_acc: 0.8991\n","Epoch: 8/20\n","938/938 [==============================] - 13s 13ms/step - loss: 0.1367 - acc: 0.9527 - val_loss: 0.3007 - val_acc: 0.8980\n","Epoch: 9/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1332 - acc: 0.9537 - val_loss: 0.3014 - val_acc: 0.8988\n","Epoch: 10/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1312 - acc: 0.9545 - val_loss: 0.3020 - val_acc: 0.8981\n","Epoch: 11/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1302 - acc: 0.9553 - val_loss: 0.3026 - val_acc: 0.8977\n","Epoch: 12/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1294 - acc: 0.9557 - val_loss: 0.3029 - val_acc: 0.8984\n","Epoch: 13/20\n","938/938 [==============================] - 13s 13ms/step - loss: 0.1289 - acc: 0.9559 - val_loss: 0.3031 - val_acc: 0.8980\n","Epoch: 14/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1285 - acc: 0.9559 - val_loss: 0.3032 - val_acc: 0.8980\n","Epoch: 15/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1285 - acc: 0.9560 - val_loss: 0.3032 - val_acc: 0.8978\n","Epoch: 16/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1285 - acc: 0.9560 - val_loss: 0.3032 - val_acc: 0.8979\n","Epoch: 17/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1284 - acc: 0.9560 - val_loss: 0.3033 - val_acc: 0.8979\n","Epoch: 18/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1285 - acc: 0.9560 - val_loss: 0.3033 - val_acc: 0.8979\n","Epoch: 19/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1283 - acc: 0.9561 - val_loss: 0.3033 - val_acc: 0.8979\n","Epoch: 20/20\n","938/938 [==============================] - 12s 13ms/step - loss: 0.1284 - acc: 0.9560 - val_loss: 0.3033 - val_acc: 0.8979\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MAHbvA_Eok-z"},"source":["## Shampoo"]},{"cell_type":"code","metadata":{"id":"ZBgeK5ttAmxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616318561205,"user_tz":-300,"elapsed":513639,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"ab230a11-af39-4cc6-9ccb-883bc4af38e1"},"source":["torch.manual_seed(1)\n","Ws = initialize_weights()\n","Qs = [[0.01*torch.eye(W.shape[0]).to(device), 0.01*torch.eye(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.5\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","TrainAcc, TestAcc = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, TrainAcc, TestAcc, times)\n","\n","def matrix_power(matrix, power):\n","    # use CPU for svd for speed up\n","    matrix = matrix.cpu()\n","    u, s, v = torch.svd(matrix)\n","    return (u @ s.pow_(power).diag() @ v.t()).cuda()\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    n = 0\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        \n","      \n","        data, target = data.to(device), target.to(device)\n","        loss, accuracy = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        trainacc += accuracy\n","        \n","        with torch.no_grad():\n","            Qs = [[q[0] + g.mm(g.t()), q[1] + (g.t()).mm(g)] for (q, g) in zip(Qs, grads)]\n","            inv_Qs = [[matrix_power(q[0], -1/4), matrix_power(q[1], -1/4)]for q in Qs]\n","            pre_grads = [q[0].mm(g).mm(q[1]) for (q, g) in zip(inv_Qs, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","        kbar.update(n, values=[(\"loss\", loss.item()), (\"acc\", accuracy.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    TrainAcc.append(trainacc.item()/n_batches)\n","    \n","    testloss, testacc = test_loss()\n","\n","    TestLoss.append(testloss)\n","    TestAcc.append(testacc)\n","    kbar.add(1, values=[(\"val_loss\", testloss), (\"val_acc\", testacc)])\n","    step_size = 0.1**(1/9)*step_size\n","    # print('Epoch: {}; train loss: {}; test loss: {}, train_accuracy: {}, test_accuracy:{}, time: {}'\\\n","    #  .format(epoch, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'shampoo.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAcc,'TestAccuracy': TestAcc, 'Time':times})"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 2.4306401169376333; test loss: 2.4298145294189455, train_accuracy: 0.10022987739872068, test_accuracy:0.1004000186920166, time: 0\n","Epoch: 1/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.4525 - acc: 0.8305 - val_loss: 0.3641 - val_acc: 0.8678\n","Epoch: 2/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.2987 - acc: 0.8873 - val_loss: 0.3181 - val_acc: 0.8825\n","Epoch: 3/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.2575 - acc: 0.9029 - val_loss: 0.3088 - val_acc: 0.8873\n","Epoch: 4/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.2348 - acc: 0.9122 - val_loss: 0.3047 - val_acc: 0.8888\n","Epoch: 5/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.2183 - acc: 0.9191 - val_loss: 0.2977 - val_acc: 0.8926\n","Epoch: 6/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.2063 - acc: 0.9234 - val_loss: 0.2919 - val_acc: 0.8955\n","Epoch: 7/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1982 - acc: 0.9257 - val_loss: 0.2921 - val_acc: 0.8963\n","Epoch: 8/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1918 - acc: 0.9285 - val_loss: 0.2939 - val_acc: 0.8964\n","Epoch: 9/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1873 - acc: 0.9300 - val_loss: 0.2934 - val_acc: 0.8978\n","Epoch: 10/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1843 - acc: 0.9315 - val_loss: 0.2933 - val_acc: 0.8977\n","Epoch: 11/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1817 - acc: 0.9326 - val_loss: 0.2929 - val_acc: 0.8985\n","Epoch: 12/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1799 - acc: 0.9330 - val_loss: 0.2935 - val_acc: 0.8979\n","Epoch: 13/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1787 - acc: 0.9337 - val_loss: 0.2939 - val_acc: 0.8971\n","Epoch: 14/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1775 - acc: 0.9340 - val_loss: 0.2942 - val_acc: 0.8978\n","Epoch: 15/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1769 - acc: 0.9344 - val_loss: 0.2941 - val_acc: 0.8970\n","Epoch: 16/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1761 - acc: 0.9346 - val_loss: 0.2943 - val_acc: 0.8976\n","Epoch: 17/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1760 - acc: 0.9347 - val_loss: 0.2943 - val_acc: 0.8975\n","Epoch: 18/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1756 - acc: 0.9346 - val_loss: 0.2944 - val_acc: 0.8979\n","Epoch: 19/20\n","938/938 [==============================] - 22s 23ms/step - loss: 0.1753 - acc: 0.9350 - val_loss: 0.2945 - val_acc: 0.8973\n","Epoch: 20/20\n","938/938 [==============================] - 22s 24ms/step - loss: 0.1751 - acc: 0.9350 - val_loss: 0.2945 - val_acc: 0.8978\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VKqHDHWs7rPo"},"source":["# Comparison"]},{"cell_type":"code","metadata":{"id":"2sNOWNfay0sr","executionInfo":{"status":"aborted","timestamp":1616315065594,"user_tz":-300,"elapsed":1040473,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["opts = ['sgd','adam','KFAC', 'shampoo','Kron','Kron_damped', 'SAMDPSGD']\n","\n","total_train_time = {}\n","opts_data = {}\n","times = {}\n","train_times = {}\n","test_times = {}\n","train_losses = {}\n","test_losses = {}\n","train_accs = {}\n","test_accs = {}\n","train_err={}\n","test_err = {}\n","\n","\n","for opt in opts:\n","\topts_data[opt] = scipy.io.loadmat(results_dir+opt+'.mat')\t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gIYMFyL-C8d","executionInfo":{"status":"aborted","timestamp":1616315065595,"user_tz":-300,"elapsed":1040464,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["colors = ['#0000FF','#00FF00','#FF0000','#33F0FF','#FFA833','#000000','#33E0FF', '#FF33E6','#D433FF','#888A0B','#8A0B1E','#B498DF','#1B786D']\n","# colors = ['#0000FF','#00FF00','#FF0000','#33F0FF','#FFA833','#FFF933','#000000','#33E0FF','#FF33E6','#D433FF','#888A0B','#8A0B1E','#B498DF','#1B786D']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Rez_eVv8XdP","executionInfo":{"status":"aborted","timestamp":1616315065597,"user_tz":-300,"elapsed":1040456,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["for opt in opts:\n","  # print(opt)\n","  data = opts_data[opt]\n","  times[opt] = data.get('Time')\n","  train_times[opt] = np.cumsum(times[opt])\n","  test_times[opt] = np.cumsum(times[opt])\n","  total_train_time[opt] = np.sum(times[opt])\n","  train_losses[opt] = data.get('TrainLoss').reshape(EPOCHS+1,)\n","  train_accs[opt] = data.get('TrainAccuracy').reshape(EPOCHS+1,)\n","  test_losses[opt] = data.get('TestLoss').reshape(EPOCHS+1,)\n","  test_accs[opt] = data.get('TestAccuracy').reshape(EPOCHS+1)\n","  train_err[opt] = (1-data.get('TrainAccuracy')).reshape(EPOCHS+1,)\n","  \n","\n","for opt in ['kron_damped','SAMDPSGD']:\n","  test_err[opt] = (1-opts_data[opt].get('TestAccuracy')).reshape(EPOCHS+1,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyfJY86HUfT0","executionInfo":{"status":"aborted","timestamp":1616315065598,"user_tz":-300,"elapsed":1040446,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["\n","fig = go.Figure()\n","i = 0\n","for opt in ['kron', 'kron_damped','SAMDPSGD']:\n","  fig.add_trace(go.Scatter(y=test_err[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","  i = i + 1\n","\n","fig.update_layout(title='test error', xaxis_title='epochs', yaxis_title='test error', yaxis_type=\"log\")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOGytOmR8w3V","executionInfo":{"status":"aborted","timestamp":1616315065599,"user_tz":-300,"elapsed":1040434,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["# plot train_losses vs Iterations\n","plot_loss_metrics(None,train_losses,'Train Loss vs EPOCHS', 'EPOCHS','Train Loss')\n","# plot test_losses vs Iterations\n","plot_loss_metrics(None,test_losses,'Test Loss vs EPOCHS', 'EPOCHS','Test Loss')\n","# # plot test_losses vs Iterations\n","plot_loss_metrics(train_times,train_losses,'Train Loss vs Time', 'Time','Train Loss')\n","# plot test_losses vs Iterations\n","plot_loss_metrics(test_times,test_losses,'Test Loss vs Time', 'Time','Test Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKKKAIyTUDue","executionInfo":{"status":"aborted","timestamp":1616315065601,"user_tz":-300,"elapsed":1040426,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["# plot train_losses vs Iterations\n","plot_loss_metrics(None,train_err,'Train Loss vs EPOCHS', 'EPOCHS','Train Loss')\n","# plot test_losses vs Iterations\n","# plot_loss_metrics(None,test_losses,'Test Loss vs EPOCHS', 'EPOCHS','Test Loss')\n","# # plot test_losses vs Iterations\n","plot_loss_metrics(train_times,train_err,'Train Loss vs Time', 'Time','Train Loss')\n","# plot test_losses vs Iterations\n","# plot_loss_metrics(test_times,test_losses,'Test Loss vs Time', 'Time','Test Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5R9mvQkv5DU","executionInfo":{"status":"aborted","timestamp":1616315065602,"user_tz":-300,"elapsed":1040417,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["# plot train_losses vs Iterations\n","plot_acc_metrics(None,train_accs,'Train Accuracy vs EPOCHS', 'EPOCHS','Train Accuracy')\n","# plot test_losses vs Iterations\n","plot_acc_metrics(None,test_accs,'Test Accuracy vs EPOCHS', 'EPOCHS','Test Accuracys')\n","# # plot test_losses vs Iterations\n","plot_acc_metrics(train_times,train_accs,'Train Accuracy vs Time', 'Time','Train Accuracy')\n","# plot test_losses vs Iterations\n","plot_acc_metrics(test_times,test_accs,'Test Accuracy vs Time', 'Time','Test Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1emXQ9-RLkxg","executionInfo":{"status":"aborted","timestamp":1616315065603,"user_tz":-300,"elapsed":1040408,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":[""],"execution_count":null,"outputs":[]}]}