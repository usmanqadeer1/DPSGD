{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"News20.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Pn7E8tXofBdm"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"3unCOQ4aeGNm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618026068221,"user_tz":-300,"elapsed":4866,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"4618e0f7-8762-44e6-aacf-19a4b995e471"},"source":["!pip install pkbar"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Collecting pkbar\n","  Downloading https://files.pythonhosted.org/packages/95/8f/28e0a21b27f836a8903315050db17dd68e55bf477b6fde52d1c68da3c8a6/pkbar-0.5-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pkbar) (1.19.5)\n","Installing collected packages: pkbar\n","Successfully installed pkbar-0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F0hq8_ykmK2J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616426103570,"user_tz":-300,"elapsed":4489,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"c93534b2-45ed-4666-c43f-7f254922dceb"},"source":["from google.colab import drive\n","from google.colab import files\n","import sys\n","import time\n","\n","drive.mount('/content/gdrive/', force_remount=True)\n","root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'Colab Notebooks/MS Thesis/PSGD Paper/'\n","results_dir = base_dir + 'RNN/results_xor/'\n","logs_dir = base_dir + 'log'\n","sys.path.append(base_dir)\n","import preconditioned_stochastic_gradient_descent as psgd \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aEZtit9cjlMG","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618026068223,"user_tz":-300,"elapsed":3735,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"1d4d3751-22c1-4166-ca9c-673f8b965450"},"source":["import matplotlib.pyplot as plt\n","import torch\n","from torch.autograd import grad\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import numpy as np\n","import time\n","import math\n","\n","import pkbar\n","from tqdm import tqdm\n","from tabulate import tabulate\n","import scipy.io\n","from sklearn import metrics\n","import plotly.express as px\n","from torchsummary import summary\n","import torch.nn as nn\n","device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","torch.cuda.get_device_name(0)\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla T4'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBtu5N2R3lQQ","executionInfo":{"status":"ok","timestamp":1618027156929,"user_tz":-300,"elapsed":1436,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"f00fc4eb-4675-453c-b806-855238208897"},"source":["model = nn.Sequential(\n","      nn.Embedding(20000, 256),\n","      nn.LSTM(256, 128),\n","      nn.Linear(128, 1)\n",")\n","[p.size() for p in model.parameters()]"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[torch.Size([20000, 256]),\n"," torch.Size([512, 256]),\n"," torch.Size([512, 128]),\n"," torch.Size([512]),\n"," torch.Size([512]),\n"," torch.Size([1, 128]),\n"," torch.Size([1])]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"QKooB49HfKzR"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"roPf15eMfNuJ"},"source":["def plot_loss_metrics(xaxis,yaxis,title, x_label,y_label):\n","\n","    fig = go.Figure()\n","    i = 0\n","    if(xaxis != None):\n","        for opt in opts:\n","            fig.add_trace(go.Scatter(x = xaxis[opt], y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","            i = i + 1\n","    else:\n","        for opt in opts:\n","            fig.add_trace(go.Scatter(y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","            i = i + 1\n","\n","    fig.update_layout(title=title, xaxis_title=x_label, yaxis_title=y_label, yaxis_type=\"log\")\n","    fig.show()\n","    fig.write_html(results_dir + title + \".html\")\n","\n","def plot_acc_metrics(xaxis,yaxis,title, x_label,y_label):\n"," \n","    fig = go.Figure()\n","    i = 0\n","    if(xaxis != None):\n","        for opt in opts:\n","            fig.add_trace(go.Scatter(x = xaxis[opt], y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","            i = i + 1\n","    else:\n","        for opt in opts:\n","            fig.add_trace(go.Scatter(y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n","            i = i + 1\n","\n","    fig.update_layout(title=title, xaxis_title=x_label, yaxis_title=y_label, yaxis=dict(range=[0.97, 1]))\n","    fig.show()\n","    fig.write_html(results_dir + title + \".html\")\n","\n","\n","def update_lambda(loss1, loss2, M, lambd, omega):\n","    \n","    r = abs(loss2 - loss1)/(M)\n","    if r > 3/4:\n","        lambd = lambd*omega\n","    elif r < 1/4:\n","        lambd = lambd / omega\n","    return lambd\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ejKz-9nfUoy"},"source":["np.random.seed(0)\n","\n","# Parameter Settings\n","BATCH_SIZE = 100\n","test_BATCH_SIZE = 100\n","EPOCHS = 10\n","GAP = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t71vTNyumHao"},"source":["# Data Download"]},{"cell_type":"code","metadata":{"id":"LyioOEA_kdvB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618027482226,"user_tz":-300,"elapsed":2560,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}},"outputId":"fc010aaf-a62a-4366-cd3c-e87d3e09167e"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from keras.preprocessing import sequence\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import numpy as np\n","\n","from sklearn.datasets import fetch_20newsgroups\n","max_features = 20000\n","\n","maxlen = 80\n","batch_size = 32\n","\n","\n","def load_20ng_dataset_bow():\n","   \n","    k=['alt.atheism','rec.autos']\n","    newsgroups_train = fetch_20newsgroups(subset='train',categories=k)\n","    newsgroups_train.target_names\n","   \n","    newsgroups_test = fetch_20newsgroups(subset='test',categories=k)\n","    newsgroups_test.target_names\n","\n","   \n","\n","    vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.95)\n","    train_data = vectorizer.fit_transform(newsgroups_train.data)\n","    test_data = vectorizer.transform(newsgroups_test.data)\n","    train_data = train_data.todense()\n","    test_data = test_data.todense()\n","    train_labels = newsgroups_train.target\n","    test_labels = newsgroups_test.target\n","\n","    return train_data, train_labels, test_data, test_labels\n","\n","   \n","\n","\n","np.random.seed(1)\n","n_train = 715\n","   \n","X_train, y_train, X_test, y_test = load_20ng_dataset_bow()\n","X_train = X_train[:n_train, :]\n","y_train = y_train[:n_train]\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","\n","\n","train_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train))\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=4)\n","\n","test_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test))\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True, num_workers=4)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"buriM2uKd80h","executionInfo":{"status":"ok","timestamp":1618027487404,"user_tz":-300,"elapsed":1229,"user":{"displayName":"M. USMAN QADEER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXrG0czXthVd59pFBGcQk02u209YXTh4NDJXhZA=s64","userId":"17493752484024501639"}}},"source":["n_batches = len(train_loader)\n","n_test_batches = len(test_loader)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O55VB__slpJx"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"kL94NHRwd80i"},"source":["# generate a random orthogonal matrix for recurrent matrix initialization \n","dim_in = 256\n","dim_hidden = 128\n","batch_size = 32\n","# initialize the RNN weights\n","\n","def initialize_weights():\n","    \n","    Wi_np = np.random.normal(loc=0.0, scale=0.1, size=[dim_in+2*dim_hidden+1, dim_hidden])\n","    Wf_np = np.concatenate([np.random.normal(loc=0.0, scale=0.1, size=[dim_in+2*dim_hidden, dim_hidden]),\n","                                    np.ones([1, dim_hidden])], 0)\n","    Wo_np = np.random.normal(loc=0.0, scale=0.1, size=[dim_in+2*dim_hidden+1, dim_hidden])\n","    Wc_np = np.random.normal(loc=0.0, scale=0.1, size=[dim_in+2*dim_hidden+1, dim_hidden])\n","    # matrix for the output layer (not for LSTM cell)\n","    W2_np = np.random.normal(loc=0.0, scale=0.1, size=[dim_hidden+1, dim_out])\n","\n","    W1 = torch.nn.init.xavier_uniform_((0.1*torch.randn(vocab_size, input_dim)).clone().detach().requires_grad_(True)).to(device)\n","    Wi = torch.tensor(Wi_np, dtype=torch.float, requires_grad=True).to(device)\n","    Wf = torch.tensor(Wf_np, dtype=torch.float, requires_grad=True).to(device)\n","    Wo = torch.tensor(Wo_np, dtype=torch.float, requires_grad=True).to(device)\n","    Wc = torch.tensor(Wc_np, dtype=torch.float, requires_grad=True).to(device)\n","    W2 = torch.tensor(W2_np, dtype=torch.float, requires_grad=True).to(device)\n","    Ws = [Wi, Wf, Wo, Wc, W2]\n","    return Ws\n","\n","def get_rand_orth( dim ):\n","    temp = np.random.normal(size=[dim, dim])\n","    q, _ = np.linalg.qr(temp)\n","    return q\n","\n","ones = torch.ones(batch_size, 1).to(device)\n","\n","\n","def model(x):\n","    Wi, Wf, Wo, Wc, W2 = Ws\n","    \n","    def lstm_cell(i, o, state):\n","        # i: input\n","        # o: output\n","        # state: cell state\n","        input_gate = torch.sigmoid(torch.matmul(torch.cat([i, o, state, ones], 1), Wi))\n","        forget_gate = torch.sigmoid(torch.matmul(torch.cat([i, o, state, ones], 1), Wf))\n","        update = torch.matmul(torch.cat([i, o, state, ones], 1), Wc)\n","        state = forget_gate*state + input_gate*torch.tanh(update)\n","        \n","        output_gate = torch.sigmoid(torch.matmul(torch.cat([i, o, state, ones], 1), Wo))\n","        o = output_gate*torch.tanh(state)\n","        return o, state\n","    \n","    state = torch.zeros([batch_size, dim_hidden]).to(device)\n","    out = torch.zeros([batch_size, dim_hidden]).to(device)\n","\n","    for xt in x.transpose(1,0):\n","        out, state = lstm_cell(xt, out, state)\n","    y = torch.matmul(torch.cat((out, ones), 1), W2)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHxAZNt3CHC2","executionInfo":{"status":"ok","timestamp":1616426111469,"user_tz":-300,"elapsed":12317,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"0e3752fe-94c3-4d61-c3af-f721df3db2df"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","initialize_weights()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[ 1.7641e-01,  4.0016e-02,  9.7874e-02,  2.2409e-01,  1.8676e-01,\n","          -9.7728e-02,  9.5009e-02, -1.5136e-02, -1.0322e-02,  4.1060e-02,\n","           1.4404e-02,  1.4543e-01,  7.6104e-02,  1.2168e-02,  4.4386e-02,\n","           3.3367e-02,  1.4941e-01, -2.0516e-02,  3.1307e-02, -8.5410e-02],\n","         [-2.5530e-01,  6.5362e-02,  8.6444e-02, -7.4216e-02,  2.2698e-01,\n","          -1.4544e-01,  4.5759e-03, -1.8718e-02,  1.5328e-01,  1.4694e-01,\n","           1.5495e-02,  3.7816e-02, -8.8779e-02, -1.9808e-01, -3.4791e-02,\n","           1.5635e-02,  1.2303e-01,  1.2024e-01, -3.8733e-02, -3.0230e-02],\n","         [-1.0486e-01, -1.4200e-01, -1.7063e-01,  1.9508e-01, -5.0965e-02,\n","          -4.3807e-02, -1.2528e-01,  7.7749e-02, -1.6139e-01, -2.1274e-02,\n","          -8.9547e-02,  3.8690e-02, -5.1081e-02, -1.1806e-01, -2.8182e-03,\n","           4.2833e-02,  6.6517e-03,  3.0247e-02, -6.3432e-02, -3.6274e-02],\n","         [-6.7246e-02, -3.5955e-02, -8.1315e-02, -1.7263e-01,  1.7743e-02,\n","          -4.0178e-02, -1.6302e-01,  4.6278e-02, -9.0730e-02,  5.1945e-03,\n","           7.2909e-02,  1.2898e-02,  1.1394e-01, -1.2348e-01,  4.0234e-02,\n","          -6.8481e-02, -8.7080e-02, -5.7885e-02, -3.1155e-02,  5.6165e-03],\n","         [-1.1651e-01,  9.0083e-02,  4.6566e-02, -1.5362e-01,  1.4883e-01,\n","           1.8959e-01,  1.1788e-01, -1.7992e-02, -1.0708e-01,  1.0545e-01,\n","          -4.0318e-02,  1.2224e-01,  2.0827e-02,  9.7664e-02,  3.5637e-02,\n","           7.0657e-02,  1.0500e-03,  1.7859e-01,  1.2691e-02,  4.0199e-02],\n","         [ 1.8832e-01, -1.3478e-01, -1.2705e-01,  9.6940e-02, -1.1731e-01,\n","           1.9436e-01, -4.1362e-02, -7.4745e-02,  1.9229e-01,  1.4805e-01,\n","           1.8676e-01,  9.0604e-02, -8.6123e-02,  1.9101e-01, -2.6800e-02,\n","           8.0246e-02,  9.4725e-02, -1.5501e-02,  6.1408e-02,  9.2221e-02],\n","         [ 3.7643e-02, -1.0994e-01,  2.9824e-02,  1.3264e-01, -6.9457e-02,\n","          -1.4963e-02, -4.3515e-02,  1.8493e-01,  6.7229e-02,  4.0746e-02,\n","          -7.6992e-02,  5.3925e-02, -6.7433e-02,  3.1831e-03, -6.3585e-02,\n","           6.7643e-02,  5.7659e-02, -2.0830e-02,  3.9601e-02, -1.0931e-01],\n","         [-1.4913e-01,  4.3939e-02,  1.6667e-02,  6.3503e-02,  2.3831e-01,\n","           9.4448e-02, -9.1282e-02,  1.1170e-01, -1.3159e-01, -4.6158e-02,\n","          -6.8242e-03,  1.7133e-01, -7.4475e-02, -8.2644e-02, -9.8453e-03,\n","          -6.6348e-02,  1.1266e-01, -1.0799e-01, -1.1475e-01, -4.3782e-02],\n","         [-4.9803e-02,  1.9295e-01,  9.4942e-02,  8.7551e-03, -1.2254e-01,\n","           8.4436e-02, -1.0002e-01, -1.5448e-01,  1.1880e-01,  3.1694e-02,\n","           9.2086e-02,  3.1873e-02,  8.5683e-02, -6.5103e-02, -1.0342e-01,\n","           6.8159e-02, -8.0341e-02, -6.8955e-02, -4.5553e-02,  1.7479e-03],\n","         [-3.5399e-02, -1.3750e-01, -6.4362e-02, -2.2234e-01,  6.2523e-02,\n","          -1.6021e-01, -1.1044e-01,  5.2165e-03, -7.3956e-02,  1.5430e-01,\n","          -1.2929e-01,  2.6705e-02, -3.9283e-03, -1.1681e-01,  5.2328e-02,\n","          -1.7155e-02,  7.7179e-02,  8.2350e-02,  2.1632e-01,  1.3365e-01],\n","         [-3.6918e-02, -2.3938e-02,  1.0997e-01,  6.5526e-02,  6.4013e-02,\n","          -1.6170e-01, -2.4326e-03, -7.3803e-02,  2.7992e-02, -9.8150e-03,\n","           9.1018e-02,  3.1722e-02,  7.8633e-02, -4.6642e-02, -9.4445e-02,\n","          -4.1005e-02, -1.7020e-03,  3.7915e-02,  2.2593e-01, -4.2257e-03],\n","         [-9.5595e-02, -3.4598e-02, -4.6360e-02,  4.8148e-02, -1.5408e-01,\n","           6.3262e-03,  1.5651e-02,  2.3218e-02, -5.9732e-02, -2.3792e-02,\n","          -1.4241e-01, -4.9332e-02, -5.4286e-02,  4.1605e-02, -1.1562e-01,\n","           7.8120e-02,  1.4945e-01, -2.0700e-01,  4.2626e-02,  6.7691e-02],\n","         [-6.3744e-02, -3.9727e-02, -1.3288e-02, -2.9779e-02, -3.0901e-02,\n","          -1.6760e-01,  1.1523e-01,  1.0796e-01, -8.1336e-02, -1.4664e-01,\n","           5.2106e-02, -5.7579e-02,  1.4195e-02, -3.1933e-02,  6.9154e-02,\n","           6.9475e-02, -7.2560e-02, -1.3834e-01, -1.5829e-01,  6.1038e-02],\n","         [-1.1889e-01, -5.0682e-02, -5.9631e-02, -5.2567e-03, -1.9363e-01,\n","           1.8878e-02,  5.2389e-02,  8.8422e-03, -3.1089e-02,  9.7400e-03,\n","           3.9905e-02, -2.7726e-01,  1.9559e-01,  3.9009e-02, -6.5241e-02,\n","          -3.9095e-02,  4.9374e-02, -1.1610e-02, -2.0307e-01,  2.0645e-01],\n","         [-1.1054e-02,  1.0202e-01, -6.9205e-02,  1.5364e-01,  2.8634e-02,\n","           6.0884e-02, -1.0453e-01,  1.2111e-01,  6.8982e-02,  1.3018e-01,\n","          -6.2809e-02, -4.8103e-02,  2.3039e-01, -1.0600e-01, -1.3595e-02,\n","           1.1369e-01,  9.7725e-03,  5.8295e-02, -3.9945e-02,  3.7006e-02],\n","         [-1.3065e-01,  1.6581e-01, -1.1816e-02, -6.8018e-02,  6.6638e-02,\n","          -4.6072e-02, -1.3343e-01, -1.3467e-01,  6.9377e-02, -1.5957e-02,\n","          -1.3370e-02,  1.0777e-01, -1.1268e-01, -7.3068e-02, -3.8488e-02,\n","           9.4352e-03, -4.2171e-03, -2.8689e-02, -6.1626e-03, -1.0731e-02],\n","         [-7.1960e-02, -8.1299e-02,  2.7452e-02, -8.9092e-02, -1.1574e-01,\n","          -3.1229e-02, -1.5767e-02,  2.2567e-01, -7.0470e-02,  9.4326e-02,\n","           7.4719e-02, -1.1889e-01,  7.7325e-02, -1.1839e-01, -2.6592e-01,\n","           6.0632e-02, -1.7559e-01,  4.5093e-02, -6.8401e-02,  1.6596e-01],\n","         [ 1.0685e-01, -4.5339e-02, -6.8784e-02, -1.2141e-01, -4.4092e-02,\n","          -2.8036e-02, -3.6469e-02,  1.5670e-02,  5.7852e-02,  3.4965e-02,\n","          -7.6414e-02, -1.4378e-01,  1.3645e-01, -6.8945e-02, -6.5229e-02,\n","          -5.2119e-02, -1.8431e-01, -4.7797e-02, -4.7966e-02,  6.2036e-02],\n","         [ 6.9846e-02,  3.7709e-04,  9.3185e-02,  3.3997e-02, -1.5682e-03,\n","           1.6093e-02, -1.9065e-02, -3.9485e-02, -2.6773e-02, -1.1280e-01,\n","           2.8044e-02, -9.9312e-02,  8.4163e-02, -2.4946e-02,  4.9495e-03,\n","           4.9384e-02,  6.4331e-02, -1.5706e-01, -2.0690e-02,  8.8018e-02],\n","         [-1.6981e-01,  3.8728e-02, -2.2556e-01, -1.0225e-01,  3.8631e-03,\n","          -1.6567e-01, -9.8551e-02, -1.4718e-01,  1.6481e-01,  1.6423e-02,\n","           5.6729e-02, -2.2268e-02, -3.5343e-02, -1.6165e-01, -2.9184e-02,\n","          -7.6149e-02,  8.5792e-02,  1.1411e-01,  1.4666e-01,  8.5255e-02],\n","         [-5.9865e-02, -1.1159e-01,  7.6666e-02,  3.5629e-02, -1.7685e-01,\n","           3.5548e-02,  8.1452e-02,  5.8926e-03, -1.8505e-02, -8.0765e-02,\n","          -1.4465e-01,  8.0030e-02, -3.0911e-02, -2.3347e-02,  1.7327e-01,\n","           6.8450e-02,  3.7083e-02,  1.4206e-02,  1.5200e-01,  1.7196e-01],\n","         [ 9.2951e-02,  5.8222e-02, -2.0946e-01,  1.2372e-02, -1.3011e-02,\n","           9.3953e-03,  9.4305e-02, -2.7397e-01, -5.6931e-02,  2.6990e-02,\n","          -4.6685e-02, -1.4169e-01,  8.6896e-02,  2.7687e-02, -9.7110e-02,\n","           3.1482e-02,  8.2159e-02,  5.2926e-04,  8.0056e-02,  7.8260e-03],\n","         [-3.9523e-02, -1.1594e-01, -8.5931e-03,  1.9429e-02,  8.7583e-02,\n","          -1.1511e-02,  4.5742e-02, -9.6461e-02, -7.8263e-02, -1.1039e-02,\n","          -1.0546e-01,  8.2025e-02,  4.6313e-02,  2.7910e-02,  3.3890e-02,\n","           2.0210e-01, -4.6886e-02, -2.2014e-01,  1.9930e-02, -5.0604e-03],\n","         [-5.1752e-02, -9.7883e-02, -4.3919e-02,  1.8134e-02, -5.0282e-02,\n","           2.4125e-01, -9.6050e-02, -7.9312e-02, -2.2886e-01,  2.5148e-02,\n","          -2.0164e-01, -5.3945e-02, -2.7567e-02, -7.0973e-02,  1.7389e-01,\n","           9.9439e-02,  1.3191e-01, -8.8242e-02,  1.1286e-01,  4.9600e-02],\n","         [ 7.7141e-02,  1.0294e-01, -9.0876e-02, -4.2432e-02,  8.6260e-02,\n","          -2.6556e-01,  1.5133e-01,  5.5313e-02, -4.5704e-03,  2.2051e-02,\n","          -1.0299e-01, -3.4994e-02,  1.1003e-01,  1.2980e-01,  2.6962e-01,\n","          -7.3925e-03, -6.5855e-02, -5.1423e-02, -1.0180e-01, -7.7855e-03],\n","         [ 3.8273e-02, -3.4242e-03,  1.0963e-01, -2.3422e-02, -3.4745e-02,\n","          -5.8127e-02, -1.6326e-01, -1.5678e-01, -1.1792e-01,  1.3014e-01,\n","           8.9526e-02,  1.3750e-01, -1.3322e-01, -1.9686e-01, -6.6006e-02,\n","           1.7582e-02,  4.9869e-02,  1.0480e-01,  2.8428e-02,  1.7427e-01],\n","         [-2.2261e-02, -9.1308e-02, -1.6812e-01, -8.8897e-02,  2.4212e-02,\n","          -8.8872e-02,  9.3674e-02,  1.4123e-01, -2.3696e-01,  8.6405e-02,\n","          -2.2396e-01,  4.0150e-02,  1.2249e-01,  6.4856e-03, -1.2797e-01,\n","          -5.8543e-02, -2.6165e-02, -1.8224e-02, -2.0290e-02, -1.0988e-02],\n","         [ 2.1348e-02, -1.2086e-01, -2.4202e-02,  1.5183e-01, -3.8465e-02,\n","          -4.4384e-02,  1.0782e-01, -2.5592e-01,  1.1814e-01, -6.3190e-02,\n","           1.6393e-02,  9.6321e-03,  9.4247e-02, -2.6759e-02, -6.7803e-02,\n","           1.2978e-01, -2.3642e-01,  2.0334e-03, -1.3479e-01, -7.6157e-02],\n","         [ 2.0113e-01, -4.4595e-03,  1.9507e-02, -1.7816e-01, -7.2904e-02,\n","           1.9656e-02,  3.5476e-02,  6.1689e-02,  8.6279e-04,  5.2700e-02,\n","           4.5378e-02, -1.8297e-01,  3.7006e-03,  7.6790e-02,  5.8988e-02,\n","          -3.6386e-02, -8.0563e-02, -1.1183e-01, -1.3105e-02,  1.1331e-01],\n","         [-1.9518e-01, -6.5989e-02, -1.1398e-01,  7.8496e-02, -5.5431e-02,\n","          -4.7064e-02, -2.1695e-02,  4.4539e-02, -3.9239e-02, -3.0461e-01,\n","           5.4331e-02,  4.3904e-02, -2.1954e-02, -1.0840e-01,  3.5178e-02,\n","           3.7924e-02, -4.7003e-02, -2.1673e-02, -9.3016e-02, -1.7859e-02],\n","         [-1.5504e-01,  4.1732e-02, -9.4437e-02,  2.3810e-02, -1.4060e-01,\n","          -5.9006e-02, -1.1049e-02, -1.6607e-01,  1.1515e-02, -3.7915e-02,\n","          -1.7424e-01, -1.3032e-01,  6.0512e-02,  8.9556e-02, -1.3191e-02,\n","           4.0476e-02,  2.2384e-02,  3.2962e-02,  1.2860e-01, -1.5070e-01],\n","         [ 6.7646e-02, -3.8201e-02, -2.2426e-02, -3.0225e-02, -3.7515e-02,\n","          -1.2262e-01,  1.8334e-02,  1.6709e-01, -5.6133e-03, -1.3850e-04,\n","          -6.8730e-02, -1.1747e-02,  4.6617e-02, -3.7024e-02, -4.5380e-02,\n","           4.0326e-02, -9.1800e-02,  2.5250e-02,  8.2032e-02,  1.3599e-01],\n","         [-9.0382e-03,  1.3676e-01,  1.0344e-01, -9.9621e-02, -1.2179e-01,\n","          -3.0496e-02,  1.0289e-01, -7.2287e-03, -6.0066e-02,  1.5522e-01,\n","           2.8690e-02, -2.3206e-01,  3.1716e-02,  5.2004e-02,  2.2561e-02,\n","           4.4971e-02, -6.7276e-03, -1.3184e-01, -3.7070e-02, -9.4562e-02],\n","         [-9.3274e-02, -1.2631e-01,  4.5249e-02,  9.7896e-03, -4.4817e-02,\n","          -6.4934e-02, -2.3423e-03,  1.0792e-01, -2.0042e-01,  3.7688e-02,\n","          -5.4571e-02, -1.8846e-01, -1.9457e-01, -9.1278e-02,  2.1951e-02,\n","           3.9306e-02, -9.3898e-02,  1.0170e-01,  1.4230e-01,  3.9609e-02],\n","         [-5.9140e-02,  1.1244e-01,  7.5540e-02,  8.6741e-02, -6.5646e-02,\n","          -2.8346e-01,  2.1168e-01, -1.6109e-01, -3.5768e-03,  2.3807e-01,\n","           3.3058e-02,  9.4925e-02, -1.5024e-01, -1.7777e-01, -5.3270e-02,\n","           1.0907e-01, -3.4625e-02, -7.9464e-02,  1.9797e-02,  1.0819e-01],\n","         [-1.4449e-01, -1.2105e-01, -7.8867e-02,  1.0946e-01,  2.3482e-02,\n","           2.1322e-01,  9.3645e-02, -3.5095e-03,  1.2651e-01,  2.1150e-02,\n","          -7.0492e-02,  6.7997e-02, -6.9633e-02, -2.9040e-02,  1.3278e-01,\n","          -1.0128e-02, -8.0314e-02, -4.6434e-02,  1.0218e-01, -5.5254e-02],\n","         [-3.8687e-02, -5.1029e-02,  1.8393e-02, -3.8549e-02, -1.6018e-01,\n","          -8.8718e-02, -9.3279e-02,  1.2433e-01,  8.1267e-02,  5.8726e-02,\n","          -5.0536e-02, -8.1579e-02, -5.0752e-02, -1.0519e-01,  2.4972e-01,\n","          -2.2453e-01,  5.6401e-02, -1.2846e-01, -1.0434e-02, -9.8800e-02],\n","         [-1.1776e-01, -1.1402e-01,  1.7550e-01, -1.3299e-02, -7.6570e-02,\n","           5.5579e-02,  1.0349e-03,  7.2003e-02, -1.8243e-01,  3.0360e-02,\n","           7.7269e-02, -1.6616e-01,  4.4820e-02,  1.6962e-01, -1.4858e-03,\n","           8.2141e-02,  6.7057e-02, -7.0751e-02,  3.9767e-03, -1.5670e-01],\n","         [-4.5130e-02,  2.6569e-02,  7.2310e-02,  2.4612e-03,  7.1998e-02,\n","          -1.1029e-01, -1.0170e-02,  1.9279e-03,  1.8496e-01, -2.1417e-02,\n","          -4.9902e-02,  2.1351e-03, -9.1911e-02,  1.9275e-02, -3.6506e-02,\n","          -1.7913e-01, -5.8587e-03, -3.1754e-02, -1.6324e-01, -6.7134e-03],\n","         [ 1.4894e-01,  5.2130e-02,  6.1193e-02, -1.3415e-01,  4.7690e-02,\n","           1.4845e-02,  5.2905e-02,  4.2263e-02, -1.3598e-01, -4.1401e-03,\n","          -7.5787e-02, -5.0084e-03, -8.9740e-02,  1.3125e-01, -8.5897e-02,\n","          -8.9894e-02,  7.4586e-03, -1.0771e-01, -4.2466e-02, -8.2996e-02],\n","         [ 1.4112e-01,  7.8580e-02, -5.7470e-03, -3.9122e-02,  9.4092e-02,\n","           4.0520e-02,  4.9805e-02, -2.6192e-03, -1.6882e-01, -1.1247e-02,\n","          -5.3249e-02,  6.4506e-02,  1.0118e-01, -6.5795e-02,  4.6839e-02,\n","           1.7359e-01, -6.6771e-02,  1.6819e-01, -8.5259e-02,  2.2960e-03],\n","         [-1.1146e-03,  1.1499e-03, -8.3768e-02, -5.9118e-02, -6.6772e-02,\n","           3.2696e-02,  3.3004e-02,  2.2259e-01,  1.3710e-01, -5.0984e-02,\n","           3.2487e-02,  9.9712e-02,  3.0602e-03, -6.9642e-03,  5.1575e-03,\n","           8.6728e-02, -8.4832e-02, -3.2567e-02,  4.7043e-02,  3.1145e-02],\n","         [ 2.3958e-02, -3.6980e-02,  9.7254e-02,  2.1339e-01,  4.0642e-02,\n","          -1.9318e-02,  7.5574e-02, -5.3913e-02, -7.4969e-02,  3.2809e-03,\n","          -2.5828e-01, -1.1540e-01, -3.4796e-02, -1.3534e-01, -1.0326e-01,\n","          -4.3675e-02, -1.6430e-01, -4.0607e-02, -5.3527e-02,  2.5405e-03]],\n","        device='cuda:0', grad_fn=<CopyBackwards>),\n"," tensor([[ 1.1542e-01,  1.7250e-02,  2.1062e-03,  9.9454e-03,  2.2739e-02,\n","          -1.0167e-01, -1.1478e-02,  3.0875e-02, -1.3708e-01,  8.6565e-02,\n","           1.0814e-01, -6.3138e-02, -2.4134e-02, -8.7819e-02,  6.9938e-02,\n","          -1.0612e-01, -2.2248e-02, -8.5892e-02,  5.0954e-03, -1.7942e-01],\n","         [ 1.3265e-01, -9.6461e-02,  5.9895e-03, -2.1252e-02, -7.6211e-02,\n","          -8.8778e-02,  9.3640e-02, -5.2564e-02,  2.7117e-02, -8.0150e-02,\n","          -6.4718e-02,  4.7225e-02,  9.3041e-02, -1.7532e-02, -1.4219e-01,\n","           1.9980e-01, -8.5655e-02, -1.5416e-01,  2.5944e-01, -4.0403e-02],\n","         [-1.4617e-01, -6.8344e-02,  3.6754e-02,  1.9031e-02, -8.5173e-02,\n","           1.8227e-01, -5.2158e-02, -1.1847e-01,  9.6069e-02,  1.3291e-01,\n","          -8.1749e-02, -1.4013e-01,  1.0304e-01, -2.0473e-01, -1.2266e-01,\n","           9.6745e-02, -5.5353e-03, -2.6394e-02,  3.5282e-02, -1.5277e-02],\n","         [-1.2987e-01,  1.2761e-01,  1.3250e-01,  2.0533e-02,  4.5134e-03,\n","           2.3396e-01, -2.7643e-02, -2.5958e-02,  3.6448e-02,  1.4713e-01,\n","           1.5928e-01, -2.5857e-02,  3.0833e-02, -1.3781e-01, -3.1198e-02,\n","          -8.4029e-02, -1.0068e-01,  1.6816e-01, -7.9229e-02, -5.3161e-02],\n","         [ 3.6585e-02,  1.2978e-01,  4.8112e-02,  2.7594e-01, -7.4668e-03,\n","           2.5872e-02,  2.7560e-02,  1.4350e-01,  5.0724e-02, -1.1623e-02,\n","          -9.4749e-02,  2.4444e-02,  1.4013e-01, -4.1038e-02,  5.2894e-02,\n","           2.4615e-02,  8.6352e-02, -8.0475e-02,  2.3466e-01, -1.2792e-01],\n","         [-3.6555e-02,  9.3809e-02,  2.9673e-02,  8.2999e-02, -4.9610e-02,\n","          -7.4805e-03,  1.2232e-03,  1.5693e-01,  6.9043e-02,  7.9667e-02,\n","          -6.5793e-02,  9.6888e-02,  2.2558e-02,  1.3891e-01,  2.0141e-01,\n","          -3.0677e-02, -4.0630e-02, -8.6405e-02, -1.4358e-02, -3.8203e-02],\n","         [ 3.5950e-02, -1.4457e-02, -3.6160e-02,  1.0646e-01, -9.3788e-02,\n","           4.3311e-02, -4.0594e-02,  7.2437e-02,  1.3853e-01, -3.0310e-02,\n","           4.4103e-02,  1.7879e-02, -7.9942e-02,  2.4079e-02,  2.8912e-02,\n","           4.1287e-02, -1.9840e-02,  9.4192e-03, -1.1476e-01, -3.5811e-02],\n","         [ 5.5596e-02,  8.9247e-02, -4.2231e-02,  1.0471e-02,  2.2805e-02,\n","           2.0148e-02,  5.4077e-02, -1.8181e-01, -4.9324e-03,  2.3903e-02,\n","          -1.0003e-01,  1.6740e-01,  1.6156e-02,  1.5634e-01, -7.9052e-02,\n","          -9.0730e-02,  2.2425e-02, -1.6787e-01,  2.1497e-02,  9.7219e-03],\n","         [ 1.0157e-01,  7.0104e-02, -4.1748e-02, -1.0975e-01,  1.7123e-01,\n","          -7.9212e-02, -1.0455e-01, -1.0849e-01,  1.1173e-01, -5.1890e-02,\n","          -7.5370e-02,  1.3769e-02, -2.0694e-02, -6.7810e-02,  7.5399e-02,\n","           1.0653e-01,  9.8532e-02,  7.6692e-02,  4.0263e-02, -1.7759e-01],\n","         [ 1.6693e-01,  3.0199e-02,  6.0816e-02,  1.1150e-01,  1.4334e-01,\n","           4.1840e-02,  4.3555e-02, -5.9922e-02,  3.3090e-03, -8.5416e-02,\n","          -7.1994e-02, -8.9357e-02, -1.5602e-02,  1.0491e-01,  3.1710e-01,\n","           1.8950e-02, -1.3484e-01,  1.2650e-01, -3.0078e-02, -6.6061e-02],\n","         [ 2.0985e-02, -1.2406e-01,  2.2246e-02, -8.8376e-03,  9.8378e-03,\n","           3.8142e-02,  6.7492e-03,  1.6338e-03,  2.8431e-02,  4.1540e-02,\n","          -1.0315e-01, -1.4300e-01, -6.1638e-03, -1.4327e-01,  8.7531e-03,\n","           9.3875e-02,  6.0711e-02, -1.0482e-01, -8.6026e-02,  3.2830e-02],\n","         [-4.0130e-02, -3.1666e-02,  5.9691e-02, -9.8729e-02, -4.0123e-02,\n","          -8.0008e-02, -1.0431e-01, -8.5708e-02,  6.7746e-02,  5.1820e-03,\n","          -8.7916e-02, -2.3110e-02, -1.6388e-01, -7.3331e-02,  2.1496e-01,\n","          -9.0244e-03,  7.3166e-02, -6.5488e-03,  3.4817e-02,  6.6326e-02],\n","         [-1.1046e-01, -3.0936e-03,  1.5789e-01, -7.9550e-02, -5.6644e-02,\n","          -3.0769e-02,  2.6902e-02,  5.2492e-02,  1.2674e-01,  4.9950e-02,\n","          -6.2053e-03,  1.2592e-01,  7.0411e-02, -1.4957e-01,  2.5264e-01,\n","           1.7699e-01, -1.6821e-02,  3.7791e-02,  1.3244e-01, -1.7220e-02],\n","         [ 7.3035e-02,  1.1046e-01, -1.0148e-01, -6.0233e-02,  9.2141e-02,\n","           4.6081e-02,  9.2380e-02, -1.3257e-02, -2.8901e-02, -1.9986e-01,\n","          -1.1460e-01,  4.7066e-03,  8.2456e-02,  5.3118e-02, -1.2824e-02,\n","          -2.7177e-02,  2.1718e-02,  7.8211e-03,  1.4045e-01,  1.4644e-02],\n","         [-1.4812e-01, -1.2726e-01,  1.5188e-01, -1.1712e-01,  7.6450e-02,\n","          -2.6837e-02, -1.6976e-02, -1.3413e-02,  1.2214e-01, -1.9284e-02,\n","          -3.3319e-03, -1.5308e-01,  2.0669e-02,  5.3104e-02,  2.3915e-02,\n","           1.3979e-01,  5.5171e-03,  2.9898e-02,  1.6485e-01, -1.5500e-01],\n","         [-4.5583e-02,  1.4262e-01,  9.3613e-02,  6.7838e-02,  8.3265e-02,\n","           3.2707e-02,  1.6316e-01,  3.7776e-02,  2.3987e-02,  1.5896e-02,\n","           1.9286e-02, -1.1570e-01,  7.7067e-02, -1.3044e-02,  1.8219e-01,\n","          -7.5650e-03,  4.2092e-02,  2.4660e-02, -6.2556e-02,  9.9214e-02],\n","         [ 1.9051e-01, -1.4777e-03, -3.0048e-02, -3.5503e-02, -1.8924e-01,\n","          -1.7781e-02,  2.5100e-02,  1.0548e-01,  9.6005e-02, -4.1650e-02,\n","          -2.7682e-02,  1.1239e-01, -1.7346e-02, -5.1003e-02,  1.3925e-01,\n","           1.0376e-01,  1.8792e-03, -5.9378e-02, -2.0119e-01,  5.8970e-02],\n","         [-8.9637e-02, -1.9627e-01,  1.5848e-01,  6.4797e-02, -1.1390e-01,\n","          -1.2144e-01,  8.7096e-02, -8.7797e-02,  1.2961e-01,  6.1646e-02,\n","           5.3660e-02,  4.0470e-02,  1.9145e-02,  8.8051e-02, -4.5408e-02,\n","           8.5952e-03,  7.5195e-02,  5.6299e-02, -1.1950e-01, -5.0041e-02],\n","         [ 2.5280e-02, -4.0801e-02,  1.7747e-01, -3.9315e-02, -1.6222e-02,\n","           7.6943e-02,  3.3053e-02, -1.4527e-02, -7.5649e-02,  3.0151e-02,\n","           1.0391e-01,  4.7910e-02, -7.7818e-02,  1.7368e-01, -1.4466e-01,\n","          -1.5827e-01,  9.6056e-02,  2.2584e-02, -5.4950e-02, -1.0986e-01],\n","         [ 2.3208e-01,  1.1709e-02,  5.3420e-02,  3.1789e-02,  4.3481e-02,\n","           5.4009e-02,  7.3242e-02, -3.7522e-02, -2.9164e-02, -1.7410e-01,\n","          -7.8030e-02,  2.7111e-02,  1.0450e-01,  5.9904e-02, -3.4069e-02,\n","          -1.2632e-01, -2.7774e-01,  1.1517e-01, -5.8923e-02, -4.4847e-02],\n","         [ 1.3157e-02, -1.4056e-01, -3.4978e-02,  2.0235e-01,  5.0539e-02,\n","           3.5925e-02, -1.5825e-01,  2.2436e-01, -1.4228e-01,  1.9223e-01,\n","          -2.1151e-01,  1.4054e-01,  1.6181e-01, -8.2441e-02,  4.2258e-02,\n","           5.4748e-02, -8.1379e-02, -1.4491e-01, -1.3177e-01,  5.4101e-02],\n","         [-8.5116e-03, -5.6430e-02,  9.6677e-02,  5.0807e-02, -7.5546e-02,\n","          -1.2012e-01,  5.2326e-02, -5.3758e-02,  9.9205e-03,  1.5763e-01,\n","           5.0233e-02, -8.6227e-02,  1.6066e-02, -9.5264e-02,  1.6085e-01,\n","          -5.6158e-02,  2.0727e-02,  3.0773e-02,  1.5925e-02, -1.9585e-01],\n","         [-1.4464e-01, -4.5235e-02,  3.1943e-02, -1.3778e-02, -9.5715e-02,\n","          -1.3484e-01, -4.0156e-02, -4.6848e-02,  5.1284e-02, -3.2632e-02,\n","           6.0271e-02, -5.9465e-02, -2.5596e-02, -3.4805e-02, -7.8237e-02,\n","           6.2512e-02, -8.1360e-02, -5.2164e-02, -7.3120e-03, -1.2974e-01],\n","         [-3.2493e-02, -7.1131e-02, -3.8815e-02, -5.9928e-03, -7.9991e-02,\n","          -2.2008e-02,  1.3087e-01, -2.5799e-03,  1.1453e-01,  3.4649e-02,\n","           7.7416e-02, -7.7446e-02,  1.0491e-02,  1.3391e-02, -6.1263e-02,\n","          -8.2283e-02, -1.4903e-01,  1.4961e-01, -9.7240e-02,  1.3462e-01],\n","         [-4.6749e-02, -8.6249e-02,  6.2252e-02, -6.3119e-02,  5.6846e-02,\n","          -3.3281e-02,  4.8042e-02, -9.6819e-02,  8.3135e-02,  4.8797e-02,\n","          -9.1965e-02,  2.6429e-01,  5.4012e-02,  2.2905e-01,  1.6003e-01,\n","          -1.8883e-02, -4.1227e-02, -4.0346e-02, -1.8300e-01, -6.9584e-02],\n","         [ 2.4677e-02,  1.5260e-01, -7.7277e-02,  8.8206e-02, -1.2526e-01,\n","          -5.8632e-02, -4.5764e-02,  3.7181e-02,  4.5731e-02,  9.6234e-02,\n","           7.7084e-02,  2.4317e-02,  3.9036e-02,  1.5885e-01, -5.1093e-02,\n","           7.7473e-02, -1.8081e-01,  4.1133e-02, -4.8325e-02,  2.5712e-04],\n","         [ 1.0401e-01,  1.6464e-02,  8.8519e-02,  1.4738e-01,  3.8909e-02,\n","           1.1710e-01, -3.2656e-02, -8.2099e-04, -5.2262e-02,  1.0430e-01,\n","           4.1409e-02, -5.0723e-02,  1.5467e-02,  1.0416e-01, -3.9268e-03,\n","          -9.4893e-02,  1.3191e-02, -1.9806e-01,  7.6877e-02, -4.2133e-02],\n","         [-4.6931e-02,  8.7570e-02, -1.3652e-01,  1.9471e-01, -4.8024e-02,\n","          -5.2325e-02,  1.0212e-01,  7.0870e-02,  2.4512e-01, -2.1121e-02,\n","          -1.2041e-02, -1.4793e-01, -3.3210e-02, -7.2143e-02, -4.4877e-02,\n","          -1.7442e-01,  1.6606e-01, -1.4166e-01, -2.8022e-01, -1.1884e-01],\n","         [-6.0384e-02, -1.1496e-01,  1.0983e-01, -1.3784e-02,  2.5386e-03,\n","           6.1039e-02,  2.8601e-02,  9.7857e-02, -1.1095e-01, -5.4752e-02,\n","           6.6597e-02, -2.5346e-01, -1.3752e-01,  5.0099e-02, -4.8025e-02,\n","           9.3611e-02,  8.0918e-02, -1.1981e-01,  4.0666e-02,  1.2017e-01],\n","         [ 1.4743e-02, -9.7746e-02,  8.7939e-02,  6.3542e-02,  5.4261e-02,\n","           7.1594e-02, -2.9946e-01,  8.8094e-02,  1.8081e-01,  4.3664e-02,\n","           1.9273e-02,  6.9644e-02,  3.3823e-02,  6.5178e-02,  1.4710e-04,\n","          -7.6670e-02, -1.0043e-01, -9.9819e-02, -1.3730e-01, -1.0677e-01],\n","         [ 1.7613e-01,  7.5410e-02, -6.2503e-02, -3.9039e-02,  1.1256e-02,\n","          -6.5554e-02,  6.7517e-03,  7.7760e-02, -3.5743e-03,  3.3602e-02,\n","           8.8649e-02, -2.7213e-02,  2.8479e-02, -3.0938e-02, -2.8529e-03,\n","          -3.2473e-02, -5.2887e-02,  1.7371e-02,  5.6655e-02,  1.4630e-02],\n","         [ 4.9873e-02, -7.3793e-02, -1.2037e-01,  4.1704e-02,  6.8788e-02,\n","           4.9857e-03,  1.3480e-01,  9.0770e-02,  2.6806e-01, -2.0081e-02,\n","          -9.9885e-02, -7.4014e-02, -5.6550e-02,  4.7603e-02, -2.1581e-01,\n","           1.3186e-01, -2.3930e-02, -2.4679e-02, -1.0793e-01, -1.1423e-02],\n","         [ 1.3240e-03, -1.2194e-02,  3.3906e-02, -5.8963e-02, -8.9582e-02,\n","           5.4833e-02,  9.8667e-03,  1.9718e-02,  1.0590e-01, -1.0226e-01,\n","          -8.5524e-02,  1.2572e-01, -1.4829e-01, -1.3094e-01,  8.1786e-02,\n","           2.3820e-02,  1.0523e-02, -9.1659e-03,  3.1268e-03, -9.2112e-03],\n","         [ 1.3554e-01, -3.9815e-02, -1.6137e-02,  1.7944e-01,  2.7510e-03,\n","           2.2320e-01, -1.0498e-02,  1.3674e-01, -1.6553e-01,  1.5364e-02,\n","          -1.5845e-01,  8.4445e-02, -1.2129e-01,  2.8377e-02, -2.8220e-02,\n","          -1.1582e-01, -1.6194e-01, -5.1104e-02,  1.7406e-01, -2.9349e-02],\n","         [ 9.1722e-02, -5.7043e-03,  8.7673e-02, -1.8269e-01, -4.0319e-02,\n","           9.4941e-02, -1.6325e-02, -8.6455e-03, -4.3046e-02,  1.1494e-01,\n","           2.9751e-02,  4.4022e-03,  6.4305e-02,  5.8822e-02,  2.1259e-02,\n","           1.5470e-01, -6.0288e-03,  2.7808e-02, -6.4295e-02,  1.5012e-02],\n","         [ 1.5878e-01, -6.4326e-02, -1.1336e-01,  9.9676e-02, -1.4877e-02,\n","           9.6004e-03, -4.5113e-03,  7.9122e-03,  8.5053e-02, -8.3912e-02,\n","          -1.0118e-01,  8.4968e-03, -1.6064e-01, -1.3731e-01,  1.8667e-01,\n","           7.5747e-02, -1.0056e-03,  1.2380e-01, -1.0406e-01, -3.1560e-02],\n","         [ 6.2345e-02,  8.9067e-02,  5.1292e-02, -2.5412e-01, -9.6808e-02,\n","           4.7707e-02, -3.5595e-02,  2.5402e-01,  9.2656e-02,  5.5808e-02,\n","          -1.1169e-01, -3.5297e-03,  2.4120e-02,  1.1278e-01,  8.8113e-02,\n","           1.0330e-01, -9.2391e-02,  1.4122e-01, -1.3804e-01, -5.3591e-02],\n","         [ 4.3077e-02, -1.4989e-02, -1.0060e-01, -8.2155e-02, -1.5483e-01,\n","           5.3197e-02,  1.2606e-01, -1.0039e-02, -4.0035e-02, -1.4723e-01,\n","           9.1320e-02,  2.2113e-01, -1.7975e-01, -1.0634e-01, -6.7959e-02,\n","          -5.6432e-02,  2.2735e-02,  1.6142e-01,  1.0086e-01,  5.2760e-02],\n","         [-7.2393e-02, -1.1196e-01, -7.9678e-02,  1.5481e-01, -6.1743e-03,\n","          -4.4684e-02, -1.8376e-02,  8.2462e-02, -1.3128e-01,  1.4149e-01,\n","           1.5648e-02, -2.1634e-02,  4.4285e-02,  2.1840e-02, -3.4420e-02,\n","          -2.5271e-02, -8.6886e-02,  6.5639e-02, -5.3199e-02, -9.5626e-02],\n","         [ 1.6586e-02,  1.3291e-01, -4.8345e-03, -6.0810e-02,  4.0390e-02,\n","           1.9367e-01, -1.4519e-01,  3.8220e-02,  2.0509e-02,  1.1615e-01,\n","           9.9091e-02, -1.8671e-02, -1.6845e-01,  8.0656e-02, -8.3519e-02,\n","          -9.4674e-02,  1.1484e-01, -9.1085e-02,  1.4028e-01,  3.3584e-02],\n","         [ 3.1912e-02,  3.0726e-02, -1.6384e-01, -1.7764e-01,  2.1555e-02,\n","           5.6801e-02,  8.2611e-03, -8.2153e-02,  1.8922e-03, -8.2034e-03,\n","          -9.5716e-02,  1.0140e-01, -1.7303e-01,  5.8874e-02,  3.8432e-02,\n","           1.0097e-01, -1.0053e-01,  1.0141e-02,  2.1712e-01,  6.6207e-02],\n","         [ 1.0058e-02,  5.3916e-02,  8.6177e-03,  2.1909e-01,  9.8364e-02,\n","          -8.5615e-03,  2.5233e-02, -3.9080e-02,  1.2099e-01, -1.4061e-01,\n","          -1.6047e-01,  1.4587e-01,  2.1531e-01,  4.6830e-02,  1.1274e-02,\n","           6.5727e-02, -6.4705e-02,  1.7124e-02,  3.8909e-03,  6.2656e-02],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00]],\n","        device='cuda:0', grad_fn=<CopyBackwards>),\n"," tensor([[-1.5580e-01, -5.0703e-02,  8.4500e-02, -6.7559e-02, -9.9336e-02,\n","           2.0421e-01,  3.8118e-03, -5.7892e-02, -1.6924e-01,  7.2935e-02,\n","           6.9914e-02, -2.9876e-02, -1.1022e-01, -2.4549e-03, -8.3586e-02,\n","          -9.4209e-02, -1.0321e-02, -1.0514e-01,  2.4665e-02,  6.0799e-02],\n","         [-8.3963e-02, -1.3682e-01,  1.5613e-01, -9.4027e-02, -6.5994e-02,\n","           2.1302e-02,  5.9937e-02, -2.5632e-02,  4.6079e-02, -4.0099e-02,\n","          -9.7117e-02,  1.4263e-01,  2.4884e-01,  1.6960e-01,  1.4181e-02,\n","           1.8334e-01,  3.5570e-02, -4.7729e-02,  4.6638e-02, -9.4393e-03],\n","         [-9.8312e-02, -8.9832e-02,  8.0205e-02, -1.8465e-01,  6.0414e-02,\n","          -1.6296e-01, -2.1212e-01, -1.8388e-01,  1.9668e-01, -1.9623e-02,\n","           8.6583e-03,  1.4193e-01,  9.3418e-02, -1.3915e-01,  8.6901e-02,\n","           1.8418e-02, -3.4168e-02,  2.4291e-03,  1.2798e-01, -8.8597e-02],\n","         [ 4.0089e-02, -9.6572e-04, -1.7972e-01, -8.0225e-02,  1.9321e-02,\n","           1.2973e-01,  1.0013e-01,  5.9721e-02, -8.1528e-02,  1.8012e-01,\n","           2.1524e-02, -1.0064e-01, -1.8290e-02,  8.9625e-02,  7.6175e-04,\n","           8.8686e-02,  1.1037e-01,  4.0053e-02, -8.5770e-02,  1.3545e-02],\n","         [ 4.5166e-03,  1.8593e-01, -1.6263e-01, -1.3482e-02, -5.8409e-02,\n","           3.3511e-02, -2.4376e-01,  1.1149e-01,  1.3748e-03, -1.8447e-01,\n","          -3.6111e-02,  6.0896e-02, -1.5914e-01,  3.2222e-04, -1.0575e-01,\n","          -5.5599e-02,  2.6738e-03,  1.8345e-02, -4.7074e-02,  2.7280e-02],\n","         [ 8.1798e-02, -2.7891e-02,  1.4316e-01,  1.4622e-01, -4.2870e-02,\n","          -6.3784e-02, -1.6642e-01, -1.2657e-02, -3.6344e-02,  7.7905e-02,\n","          -1.5097e-01, -2.7739e-02,  9.6874e-02, -7.3036e-02, -7.6236e-02,\n","          -1.4469e-01,  2.6206e-01, -7.4747e-02, -1.3003e-01, -8.0385e-02],\n","         [-7.7430e-02, -2.6939e-02,  8.2537e-02, -2.9832e-02, -9.2282e-02,\n","          -1.4513e-01,  2.1857e-03,  4.2539e-03,  1.5309e-01,  9.2448e-03,\n","          -9.9008e-03, -1.0507e-01, -3.0595e-02, -4.3847e-02, -3.7016e-02,\n","          -9.5926e-02,  5.3833e-02, -1.4245e-02, -2.0035e-02, -1.7140e-01],\n","         [ 4.9364e-02,  4.8702e-02, -8.3913e-02,  9.9012e-02, -1.3648e-01,\n","          -2.1870e-03, -2.7121e-02, -1.3172e-01,  1.8970e-02,  1.7026e-01,\n","           6.7634e-03, -4.6302e-02,  4.4702e-02,  1.0572e-02,  2.7762e-03,\n","          -4.2554e-02,  1.4220e-01,  4.5636e-02, -5.2867e-02, -1.0800e-02],\n","         [-7.4087e-02, -6.0829e-02, -6.4073e-02, -1.1343e-01,  7.7728e-02,\n","          -2.9104e-02,  5.5413e-02, -6.7013e-02, -6.0362e-03, -7.1104e-02,\n","           7.1967e-02, -2.4842e-02, -7.3087e-02, -1.6417e-01,  2.7567e-02,\n","          -7.0839e-02, -1.5779e-03, -4.9173e-02,  9.5419e-02,  5.4414e-02],\n","         [ 4.4721e-02, -6.1612e-02,  4.6629e-02,  1.7148e-01, -8.3219e-02,\n","           1.7234e-02, -1.6492e-01,  1.3986e-01, -3.9791e-02,  7.8258e-02,\n","          -1.7232e-01,  1.7975e-01, -3.5687e-02,  5.4566e-02,  1.5082e-02,\n","          -2.5547e-02,  1.6858e-01, -1.6480e-01,  2.9871e-02,  9.1065e-02],\n","         [-2.9856e-03, -1.1817e-02, -1.4269e-02, -1.2276e-01,  3.8127e-03,\n","           5.1272e-02,  6.8599e-03, -2.7228e-02, -4.8973e-02, -2.7930e-02,\n","           1.2577e-01, -2.0866e-01,  4.0071e-03, -3.2775e-02,  1.4558e-01,\n","           5.5492e-03,  1.4849e-01, -2.1239e-01,  4.5958e-02,  2.8006e-02],\n","         [ 1.3905e-01, -1.6413e-01, -1.5504e-02,  6.6060e-03, -4.9580e-02,\n","           1.2166e-01, -3.3868e-02,  2.0348e-01,  1.0542e-01,  9.5083e-02,\n","           5.5930e-02, -1.0637e-01, -4.3110e-02,  5.7275e-02,  6.7756e-02,\n","           1.3072e-01, -4.6744e-02, -8.6015e-02,  8.5910e-02, -8.0963e-02],\n","         [ 8.7331e-02,  1.1997e-01,  4.5615e-02, -3.5758e-02,  4.1082e-03,\n","           5.9347e-02,  1.0186e-03,  2.1983e-01, -9.9067e-02, -1.0027e-01,\n","          -9.7690e-02, -5.8958e-02, -2.1789e-01, -6.2965e-02, -6.5328e-02,\n","           7.8514e-03,  4.1780e-02, -1.2402e-01,  9.0005e-02,  1.8022e-01],\n","         [-2.0829e-02,  1.5744e-01,  1.9899e-02,  1.9887e-01,  1.1173e-01,\n","          -1.5639e-01,  1.8627e-03,  1.0543e-01,  3.0547e-03, -3.6884e-03,\n","           1.2698e-01, -7.0985e-02,  1.7516e-03,  3.2363e-02, -3.3379e-02,\n","          -2.0129e-03,  7.7502e-02,  4.3284e-02, -8.0872e-02, -1.1041e-01],\n","         [-7.8910e-02,  1.2485e-04, -1.5994e-02, -8.3196e-02, -5.9815e-02,\n","          -1.5200e-01,  4.1785e-02, -4.0019e-03, -1.2598e-01,  2.8621e-03,\n","           1.3426e-01, -7.3994e-02,  1.3151e-01, -3.2346e-02,  1.9783e-02,\n","           9.7751e-03,  1.4015e-01,  1.5843e-02, -1.1419e-01, -1.3110e-01],\n","         [-1.5329e-01, -1.7120e-01,  4.6135e-03, -9.5837e-02, -8.0812e-03,\n","          -7.0386e-02, -7.7078e-02, -4.8085e-02,  7.0359e-02,  9.2915e-02,\n","           3.7117e-02, -9.8982e-02,  6.4363e-02,  6.8890e-02,  2.7465e-02,\n","          -6.0362e-02,  7.0886e-02,  4.2282e-02, -3.1169e-01,  6.4445e-02],\n","         [-1.9137e-01,  6.6356e-02, -1.5407e-02,  1.1936e-01, -9.8161e-03,\n","          -8.8661e-02, -1.4735e-02,  1.0598e-01,  2.6247e-03, -1.1434e-02,\n","           7.4355e-02,  2.1036e-02, -5.9274e-04,  1.3661e-01,  1.5551e-01,\n","           6.1333e-02, -2.8596e-02,  1.4969e-01,  1.1831e-01,  7.1890e-02],\n","         [-1.2161e-01,  1.4067e-02, -7.4367e-02, -1.5901e-02,  2.4006e-02,\n","           1.0016e-02, -4.7518e-02,  1.2730e-01, -1.6961e-01,  7.3018e-02,\n","          -1.8575e-01,  3.8260e-02, -8.8690e-02,  8.7830e-02,  8.6453e-03,\n","           2.4771e-02, -1.0183e-01, -6.5457e-02,  2.0722e-02,  5.8357e-02],\n","         [ 2.9291e-01,  2.2286e-02,  9.7604e-02, -1.5569e-01, -1.3299e-01,\n","          -3.5549e-02, -1.1974e-01,  1.4864e-01, -4.1022e-02,  1.3822e-01,\n","           1.4868e-01,  4.2780e-03,  5.0180e-02, -5.6099e-03,  5.3844e-02,\n","           4.8334e-02, -1.2365e-02,  5.0497e-02,  1.7237e-01,  7.1302e-02],\n","         [ 3.2580e-02,  1.2477e-02, -1.0127e-01, -1.0273e-01,  3.2336e-02,\n","          -1.3694e-01, -7.6633e-02,  1.2815e-01,  1.9142e-01, -1.6660e-01,\n","           1.6266e-01, -2.1144e-02, -1.5005e-03, -1.1341e-02,  1.0805e-01,\n","          -1.6077e-01,  4.5616e-02, -9.4487e-02,  5.7079e-02,  1.5428e-01],\n","         [-4.1733e-05,  3.7416e-02,  4.0955e-02, -7.9959e-02,  1.5116e-01,\n","           1.7065e-01,  7.0178e-02,  7.3285e-03, -4.6189e-02, -6.2649e-02,\n","           1.7108e-01,  1.4144e-01, -6.3661e-03, -1.5799e-01, -2.8320e-01,\n","          -1.0834e-01, -1.3062e-02,  1.4007e-01, -6.5166e-02,  5.0482e-02],\n","         [ 1.3032e-01,  1.2854e-02, -1.4245e-02, -1.3088e-01, -1.2025e-01,\n","           4.1610e-02, -2.0091e-02,  1.2253e-02, -4.7278e-03,  6.6414e-02,\n","          -7.8469e-02, -3.3558e-02,  1.8962e-01, -7.9979e-02, -2.8158e-02,\n","          -5.8939e-02,  4.4478e-02,  1.0224e-01, -4.9821e-02, -4.3141e-02],\n","         [-2.7898e-02,  5.2983e-02, -7.3940e-02, -3.7596e-02, -2.3722e-01,\n","          -1.3817e-01, -1.1244e-02,  8.9786e-02,  2.9508e-02, -1.0988e-01,\n","          -1.4003e-01,  1.7468e-02, -1.6528e-01,  1.0659e-01,  6.3896e-03,\n","          -1.6073e-01, -9.6595e-02, -7.2431e-02, -7.7319e-02, -1.4899e-01],\n","         [-8.7466e-02, -6.8440e-02, -7.1129e-02,  1.1280e-01,  1.0483e-02,\n","          -9.9326e-02, -3.3462e-02, -8.7956e-02, -3.0001e-02,  8.7551e-02,\n","           2.5227e-02,  2.2856e-01,  3.7593e-02, -9.1359e-02,  8.0974e-02,\n","           1.0799e-01,  1.0942e-01, -1.0942e-01, -1.4764e-02,  1.1318e-01],\n","         [-1.6847e-01, -4.9942e-02, -1.4269e-01, -9.3257e-02, -1.0125e-01,\n","           1.2506e-01, -2.3454e-02, -8.6336e-02, -1.0356e-01,  1.4167e-02,\n","          -1.1136e-03,  1.3441e-01,  5.0002e-02, -1.4318e-01, -6.2898e-02,\n","           1.0701e-01, -6.2108e-02,  1.7346e-01, -1.0983e-01,  5.7261e-02],\n","         [-8.6122e-02, -5.0960e-02,  1.0986e-01, -1.2707e-02,  8.1345e-02,\n","           4.7329e-02,  7.5387e-02, -8.8819e-02, -2.2157e-02,  4.2425e-02,\n","          -8.4907e-02,  1.6295e-01, -7.7723e-02, -3.0000e-02, -1.0066e-01,\n","          -2.1433e-01,  1.7969e-01, -2.0434e-02, -4.4791e-02, -1.9872e-02],\n","         [ 1.4199e-01, -9.6511e-02,  6.7957e-02, -4.2379e-02, -5.9667e-02,\n","           5.6706e-02,  9.8824e-02, -5.1390e-02, -7.6885e-02, -1.1691e-01,\n","           1.1035e-01, -5.7526e-02, -1.8491e-01,  1.4100e-01, -1.3699e-01,\n","           7.7946e-02,  1.8343e-02,  2.8792e-02, -5.8438e-02,  3.6559e-02],\n","         [-1.6678e-01,  5.8804e-02,  1.5570e-01,  8.8403e-02, -2.0195e-01,\n","          -9.8421e-02, -1.8779e-02,  4.8694e-02, -1.0665e-02, -4.9321e-02,\n","           5.9530e-02,  1.1642e-01, -2.3229e-02,  7.2893e-02, -2.5791e-01,\n","          -9.3751e-02, -3.2126e-02, -4.8857e-02,  3.3280e-02,  1.0138e-01],\n","         [ 5.0667e-02, -6.2223e-02, -1.5228e-01,  5.5696e-02, -1.8382e-01,\n","           6.5304e-02, -1.8845e-02, -1.1758e-01,  2.8726e-02, -2.8761e-04,\n","          -3.6597e-03, -8.4223e-03,  4.1952e-02,  9.2443e-02,  4.9662e-02,\n","           1.0121e-01, -4.4140e-03,  1.6185e-01,  5.7111e-02, -5.4369e-02],\n","         [-1.0939e-01,  2.0580e-02, -1.3065e-01, -9.7338e-02,  2.3909e-02,\n","          -6.0789e-02, -9.3332e-02, -3.4475e-03,  7.2678e-03, -2.0583e-02,\n","          -3.7755e-02,  8.5464e-02,  3.4243e-02, -2.2343e-02,  2.4643e-01,\n","           1.9383e-02,  1.1320e-01, -5.6098e-02, -1.3629e-01, -7.9176e-02],\n","         [-2.6801e-02, -4.9661e-02,  1.3364e-01, -1.2004e-02,  4.6147e-02,\n","          -4.6481e-03, -4.3355e-02,  3.7996e-03,  1.7141e-01, -7.6795e-02,\n","           7.6699e-02, -1.0260e-01, -4.5963e-02,  3.5832e-04,  3.2638e-02,\n","           1.4831e-01, -5.0083e-03, -8.4362e-02,  6.5004e-02, -3.6417e-02],\n","         [ 2.3868e-02, -1.1622e-02, -1.9435e-01,  5.0830e-02,  5.8337e-02,\n","           9.2660e-02,  1.8005e-01, -1.1951e-01,  5.1651e-02,  4.0930e-02,\n","          -4.1908e-02,  3.9711e-02,  4.9965e-02, -1.2187e-01,  2.4622e-02,\n","          -9.1798e-02, -6.5186e-02, -1.7747e-01, -4.7336e-02, -2.0357e-02],\n","         [ 5.4986e-02,  8.9993e-05, -1.5423e-01,  8.6215e-02, -1.1859e-02,\n","           4.8837e-02,  9.6594e-02,  1.4226e-01,  1.9612e-01, -7.2239e-03,\n","           3.1112e-02, -1.0784e-01,  1.0616e-01, -1.1849e-01, -1.8053e-01,\n","           8.3039e-02, -5.2170e-02,  7.7761e-02,  4.0807e-02, -1.6300e-01],\n","         [-2.7197e-01, -1.0966e-01,  1.6491e-03, -1.2218e-01, -6.5276e-02,\n","          -1.4589e-01,  1.6988e-02,  9.0826e-03, -4.8139e-02,  1.3971e-01,\n","           1.4977e-01,  5.6527e-02, -1.7998e-01, -1.1047e-01,  4.0713e-02,\n","          -6.2856e-02, -4.8709e-02,  8.9897e-02,  5.1087e-02,  1.3142e-01],\n","         [-4.2921e-02,  1.3752e-01, -5.5413e-02,  1.4995e-01,  1.0583e-02,\n","          -8.6051e-02, -1.6312e-01, -3.0147e-02, -2.5623e-02,  8.5766e-02,\n","          -1.1059e-02, -4.3243e-02,  1.0770e-01, -2.2483e-02, -5.7624e-02,\n","           5.7461e-02, -4.8983e-02,  6.5880e-02, -5.9692e-02, -2.2296e-02],\n","         [ 1.5218e-02, -3.7413e-02, -1.3451e-03,  8.1547e-02,  4.1060e-02,\n","           4.8097e-02, -6.3543e-02,  8.5283e-02,  6.6956e-02,  1.0044e-01,\n","          -7.2637e-02, -1.7246e-02,  6.3353e-02, -6.0882e-02, -2.2612e-02,\n","           1.9258e-01,  1.9518e-01,  1.2399e-01,  9.3859e-02, -1.0193e-01],\n","         [ 5.1256e-02, -3.5912e-02, -1.0586e-01, -5.0901e-02,  1.1567e-02,\n","          -5.4736e-02, -5.5080e-02,  7.9204e-02,  1.4411e-02,  2.3346e-02,\n","           1.1187e-02, -6.7570e-02, -1.3706e-01,  3.1056e-02, -5.0704e-02,\n","          -2.0108e-01, -3.9257e-02, -1.0922e-01,  6.9865e-02,  5.2163e-02],\n","         [ 4.9689e-02, -6.6504e-02,  7.3155e-02,  3.1965e-02, -4.0985e-02,\n","          -4.5334e-02,  8.9271e-02, -4.7360e-02,  3.0366e-02,  1.0340e-01,\n","           1.9093e-01,  1.6639e-01,  9.0082e-02, -1.5059e-01, -6.8905e-02,\n","          -5.4809e-02,  1.6531e-01, -6.9932e-02,  3.8617e-02,  1.0087e-02],\n","         [-9.3513e-02,  3.8182e-02,  3.9830e-02, -1.2558e-01,  1.2229e-01,\n","          -2.0865e-01, -5.9076e-02,  9.7197e-02, -1.1933e-01,  3.5027e-02,\n","          -1.2964e-01, -9.3024e-03, -2.3138e-01, -8.4257e-02, -1.5429e-01,\n","          -4.0176e-02, -4.1523e-02, -6.7366e-02,  7.9791e-02, -8.8688e-02],\n","         [ 6.3439e-02,  1.6293e-01,  1.3906e-02, -8.5767e-02, -1.2493e-01,\n","          -7.0979e-02,  7.0464e-02,  1.5559e-02,  9.3680e-02,  7.7033e-02,\n","           1.4081e-02,  4.7349e-02,  1.8552e-01,  1.4157e-01, -3.0275e-02,\n","           9.8968e-02,  5.8585e-02,  1.1364e-01,  6.7162e-02, -9.7417e-02],\n","         [-1.6197e-01,  5.7263e-02,  1.9026e-01, -7.7566e-02, -1.8809e-02,\n","          -1.0357e-01,  1.1778e-01, -2.3052e-01, -2.2637e-01,  3.7502e-02,\n","          -8.2344e-03, -4.7962e-02, -3.0109e-02,  5.3699e-02, -4.1380e-02,\n","          -1.0969e-01, -9.2736e-02,  8.8834e-02, -5.2474e-02, -1.3853e-01],\n","         [ 1.0218e-02,  5.0499e-02,  1.3290e-01,  2.1790e-02, -6.5971e-02,\n","           4.7401e-02,  7.2717e-02, -3.8905e-03, -4.4599e-03,  2.6013e-02,\n","          -6.9856e-03,  2.5011e-02, -1.0219e-01, -1.1504e-01, -8.3611e-02,\n","           6.4221e-02,  2.5880e-02,  1.0402e-01, -1.8669e-02, -1.1436e-01],\n","         [ 1.1446e-01, -1.8767e-03,  1.2835e-01,  5.9795e-02,  2.1886e-01,\n","          -2.1977e-02,  9.0072e-02,  8.9136e-02, -5.5513e-02, -1.7248e-02,\n","          -1.4617e-01, -1.5488e-01,  1.2657e-02,  7.9301e-02,  6.3802e-02,\n","           3.4002e-02,  8.6302e-02, -5.8970e-02, -2.7253e-02,  7.3752e-02]],\n","        device='cuda:0', grad_fn=<CopyBackwards>),\n"," tensor([[ 4.3312e-02, -2.1019e-02,  1.3208e-01, -1.2920e-01, -5.1868e-02,\n","          -2.8340e-02,  8.1653e-02,  2.3852e-04, -1.2615e-01,  5.1400e-02,\n","           1.0875e-01,  7.3930e-02,  6.1915e-02, -1.8743e-01, -8.9989e-02,\n","           4.8208e-02, -5.4888e-03,  5.2256e-02, -1.2663e-01, -6.1495e-03],\n","         [-1.3898e-01, -1.9537e-01,  2.9578e-02,  8.4259e-02,  2.4562e-02,\n","          -3.2996e-03, -1.5620e-01,  1.0061e-01, -4.4045e-03,  1.9596e-01,\n","           9.4231e-02, -2.0051e-01,  7.5505e-02, -1.3965e-01, -7.5950e-02,\n","          -2.5076e-02, -9.4062e-03,  3.9757e-02, -1.0229e-01, -1.1507e-01],\n","         [ 6.0061e-02, -1.3250e-03,  1.7437e-02, -2.1937e-01, -1.7714e-02,\n","          -8.9073e-02, -9.2063e-02,  9.2193e-02, -1.0957e-01, -1.0929e-01,\n","          -3.3101e-02,  4.5029e-02, -8.8401e-02,  1.2341e-01,  1.4498e-01,\n","          -8.8145e-02, -2.4508e-02, -7.7868e-02, -1.6854e-01,  3.0301e-02],\n","         [ 7.3359e-02,  2.0119e-01, -8.9741e-02,  1.3362e-01,  1.3424e-01,\n","           1.9785e-02,  6.0216e-02,  8.7327e-02,  1.9741e-01,  4.7781e-02,\n","          -6.0138e-03, -8.6617e-02,  3.0532e-02,  1.0242e-01,  2.4461e-02,\n","          -7.7992e-02,  8.9076e-03, -1.2915e-02,  2.6474e-02, -1.6618e-01],\n","         [ 5.5079e-02,  5.9542e-02,  4.4485e-02, -3.7628e-04, -1.8059e-01,\n","          -1.9323e-03,  1.0607e-01, -8.6013e-02, -1.9893e-01, -1.5406e-01,\n","           3.1403e-02,  3.7288e-02,  8.8629e-02, -5.5259e-03, -1.5003e-01,\n","          -8.1850e-02,  8.1884e-02,  1.4050e-02,  6.4983e-02,  4.3479e-02],\n","         [-2.0496e-02, -1.7401e-02,  1.8571e-01,  4.1467e-02, -1.2859e-02,\n","           4.5542e-02,  2.2291e-02, -2.1574e-01,  6.5008e-02,  1.8209e-01,\n","          -7.8028e-02,  1.4540e-01, -2.5687e-02,  2.9347e-02,  1.0704e-01,\n","          -7.2000e-02,  1.2425e-01, -1.2142e-01, -8.7515e-02, -5.9352e-02],\n","         [ 6.6201e-02, -3.4087e-02, -1.5200e-01, -2.1653e-02, -7.8422e-02,\n","           7.3129e-02, -3.4324e-02,  7.0774e-03, -4.0547e-02,  4.3394e-02,\n","          -1.8359e-02,  3.2520e-02, -2.5934e-01,  9.7251e-03,  4.1391e-02,\n","          -1.9928e-02,  6.6939e-02,  7.3861e-02,  1.3042e-01,  1.0481e-02],\n","         [-1.9138e-01, -2.2855e-01, -1.6018e-01, -3.7907e-03, -1.5731e-02,\n","           2.7624e-02, -6.2525e-02, -7.3649e-02,  5.5505e-02,  6.5592e-02,\n","          -2.5665e-02, -3.8477e-03,  4.0431e-02,  5.0434e-02, -1.1440e-01,\n","          -7.1957e-02, -1.2305e-01, -5.0691e-02,  8.1233e-02,  5.4627e-02],\n","         [-1.0981e-01,  5.1227e-02,  8.5843e-03, -4.9393e-02, -1.4065e-01,\n","          -1.7482e-02,  6.7994e-02, -2.1631e-01, -3.9612e-02,  2.2543e-01,\n","           6.7264e-02,  2.5983e-02, -7.3719e-02, -6.7833e-02, -8.3288e-03,\n","           1.6029e-01,  4.6559e-02, -8.7216e-02,  1.1768e-01, -2.9259e-02],\n","         [ 1.6973e-01, -5.6660e-02, -1.0033e-01,  1.7463e-02,  9.8233e-02,\n","           1.0374e-01,  1.5919e-02, -9.8810e-02, -5.0534e-02, -2.0183e-01,\n","          -9.1312e-02, -1.7846e-02,  3.8900e-02, -3.3945e-02, -5.6979e-03,\n","          -3.9619e-02,  7.5103e-02, -8.9911e-02,  8.3755e-02,  1.9609e-01],\n","         [ 4.7279e-02, -5.2709e-02, -5.3627e-02,  1.2098e-01, -1.1266e-01,\n","          -9.5380e-02, -1.1644e-01, -1.2785e-01, -1.0448e-01,  7.8990e-02,\n","           1.1023e-01, -6.9707e-02,  2.0733e-02,  7.5916e-02,  1.0056e-02,\n","          -9.5494e-02, -1.4704e-01,  1.0104e-01,  4.9618e-02,  5.7696e-02],\n","         [-1.1076e-01,  2.3498e-02,  6.2900e-02,  3.1403e-02, -7.4502e-02,\n","           1.0123e-01, -1.5276e-01,  9.2874e-02,  1.0811e-01,  1.5723e-01,\n","          -3.4249e-02, -9.9943e-02,  7.9388e-02, -6.9922e-02,  4.3996e-03,\n","          -3.1746e-02, -9.0207e-02,  3.2100e-02, -1.3920e-01,  5.9221e-02],\n","         [-9.6693e-02, -1.7317e-01, -5.0107e-03,  4.3163e-02,  5.7693e-02,\n","           8.1835e-02, -2.3536e-01, -1.0051e-01,  1.0665e-02,  1.5190e-01,\n","           7.8374e-02,  1.9013e-01, -5.2494e-02,  2.7442e-02, -1.1000e-01,\n","          -4.0435e-02, -7.3530e-02, -6.3399e-02, -3.9345e-02,  2.7175e-04],\n","         [ 2.2213e-03,  5.4345e-02,  1.3999e-02, -3.4405e-02, -5.2258e-02,\n","          -3.0713e-02, -4.4904e-02,  4.9097e-02,  8.6553e-02,  1.2740e-01,\n","          -7.9770e-02,  4.6937e-02, -1.3947e-01,  3.7317e-02,  1.0827e-01,\n","          -1.4959e-02,  1.0726e-01, -1.1386e-01, -8.8865e-02, -1.3581e-02],\n","         [ 1.0222e-01, -4.1743e-02, -4.5355e-02, -9.9163e-02,  2.0288e-02,\n","           1.2467e-01,  7.0068e-02,  6.9665e-02, -2.0697e-02, -5.6331e-02,\n","           6.7725e-02, -3.1911e-03, -1.7361e-02,  8.9824e-02, -1.9779e-02,\n","          -8.3778e-02,  9.0919e-02,  8.0720e-03, -1.0370e-01, -1.1129e-01],\n","         [ 9.5412e-03,  2.3374e-01, -3.9282e-02, -3.3627e-02,  1.5238e-01,\n","          -5.7281e-03, -1.4485e-01, -1.5728e-01,  1.2267e-01,  6.6635e-02,\n","           8.2613e-02, -5.7757e-03, -7.2671e-02, -2.1716e-02,  1.3603e-02,\n","          -8.3831e-02,  5.6145e-02, -1.2596e-01, -3.3276e-02, -2.0401e-02],\n","         [-6.9102e-02, -2.2055e-01,  4.4787e-02, -7.5575e-02,  1.3257e-01,\n","          -3.4198e-02, -5.4136e-02,  9.1522e-03,  1.0534e-01, -5.6341e-02,\n","           1.0147e-01,  1.4403e-01,  9.9032e-02,  1.6264e-01,  1.2926e-01,\n","           1.5149e-01,  1.6043e-01,  2.0807e-02, -4.2922e-02, -2.2622e-01],\n","         [-1.3227e-01, -4.4828e-02, -3.8174e-02, -1.5279e-02, -1.0008e-01,\n","          -1.5958e-01, -1.3022e-02, -1.8942e-02, -8.0755e-02, -7.4215e-02,\n","          -9.4016e-02, -3.9652e-02, -8.5630e-02,  1.2599e-01,  2.4100e-02,\n","          -9.7232e-02, -2.8045e-02, -1.1803e-01,  1.0122e-01,  1.3842e-01],\n","         [ 1.2520e-01, -1.1447e-01, -9.1267e-03, -4.0157e-02,  5.6201e-02,\n","          -1.0079e-01, -6.7589e-02, -4.1322e-02,  1.5329e-02,  6.9413e-02,\n","          -3.2873e-02,  6.6397e-02,  8.2208e-02, -2.1322e-02, -1.2457e-01,\n","          -1.1712e-01,  5.9173e-02, -4.7622e-02, -1.7126e-01,  6.1295e-02],\n","         [ 1.2955e-02, -1.4060e-01,  1.1794e-01,  8.3664e-02,  1.3875e-02,\n","          -1.2743e-01, -1.4023e-01, -3.0707e-02, -1.7139e-01,  4.0508e-02,\n","          -1.4108e-01,  1.6491e-02, -2.8813e-02,  7.1179e-02, -9.3795e-02,\n","           2.7373e-02, -1.3948e-01,  7.9555e-02, -1.1496e-02,  4.9585e-02],\n","         [-1.3205e-01,  4.9908e-02,  3.0620e-02,  3.6370e-02,  3.1263e-02,\n","          -1.9346e-02,  1.2413e-01, -1.5590e-02, -7.3917e-02, -5.8726e-03,\n","          -9.5052e-02, -4.6400e-02, -1.7725e-02, -3.7955e-02,  1.9940e-02,\n","           1.9458e-01,  5.7095e-02,  1.0723e-01, -5.0371e-02, -5.8702e-02],\n","         [-3.7818e-02,  8.5289e-02, -2.1481e-01, -1.0332e-01,  1.0234e-02,\n","          -2.2409e-02,  1.9677e-01,  4.4768e-02, -6.6219e-02, -1.5776e-01,\n","          -3.4056e-02, -1.3032e-01,  4.6675e-02,  1.6111e-02,  3.2003e-02,\n","           2.0792e-01, -9.0747e-02, -1.9240e-02, -1.2125e-01, -8.0599e-03],\n","         [ 1.5933e-01,  5.6872e-02, -1.1449e-02,  2.5163e-02, -1.2109e-01,\n","          -3.9373e-02,  8.5253e-03,  9.9422e-03, -1.5306e-01,  3.2762e-02,\n","           2.7920e-02, -3.7705e-02,  4.1750e-04, -1.4835e-01, -1.4798e-01,\n","           1.3469e-02, -6.6772e-02, -1.1556e-03,  8.3949e-02, -1.7393e-02],\n","         [-2.8107e-01, -1.5065e-02, -4.8104e-02, -2.3469e-02,  8.9973e-02,\n","          -1.5785e-01,  2.4396e-02,  1.5703e-01, -6.2594e-02,  4.7233e-02,\n","           9.6631e-02,  2.1023e-02, -6.8510e-02, -7.0952e-02,  7.4380e-02,\n","           5.9215e-02, -7.8647e-02, -1.1765e-01, -1.2808e-01,  1.6617e-01],\n","         [-6.7945e-03,  2.3602e-01,  5.5555e-02,  4.3952e-02,  3.0627e-02,\n","           9.9915e-02, -9.6606e-02,  2.1600e-01, -1.0030e-02, -7.0340e-02,\n","           3.0256e-02,  1.0923e-01, -1.0076e-01,  5.6687e-02, -7.1644e-02,\n","          -5.0627e-02, -4.8948e-02,  7.6354e-02, -1.1091e-01,  1.9262e-02],\n","         [-3.4342e-02, -8.4721e-02, -1.2135e-01, -1.2029e-01, -1.6338e-01,\n","           8.9617e-02, -2.4165e-02,  1.5865e-02,  1.1782e-01, -1.2201e-01,\n","          -9.4155e-02,  2.5472e-02, -1.8241e-01, -5.7871e-02, -9.2489e-02,\n","           3.2952e-02, -4.2582e-02,  2.0081e-01,  9.3789e-02, -8.5324e-02],\n","         [-3.8731e-02, -3.4758e-02,  3.3066e-01, -1.5102e-01,  2.0354e-02,\n","          -2.0844e-01, -6.9374e-04,  1.9099e-01, -4.0846e-02,  1.1046e-01,\n","          -6.6115e-03, -4.2250e-02, -2.5166e-02, -5.8690e-02, -6.2606e-02,\n","          -1.3302e-01,  1.5068e-01, -3.9308e-02,  2.9377e-02, -8.7653e-02],\n","         [ 1.1170e-01, -2.7356e-02, -9.1033e-03, -1.8290e-01,  3.9598e-02,\n","           1.8115e-01, -8.6908e-02, -4.5823e-02, -1.1383e-01,  1.2916e-02,\n","           6.4024e-03,  7.0508e-02,  5.5147e-02, -8.1252e-02,  2.2495e-02,\n","          -3.2830e-02, -1.0910e-01, -1.2686e-02,  3.8017e-01,  2.3152e-01],\n","         [ 1.3983e-02,  1.7389e-01, -4.5383e-03, -5.3138e-03, -1.9496e-01,\n","          -9.6011e-02, -7.8350e-02,  1.0752e-02,  1.3985e-03, -5.7894e-02,\n","          -5.8881e-02, -1.6615e-02, -1.3814e-01, -6.1264e-02, -3.8129e-02,\n","          -1.2489e-01, -3.3024e-02, -8.3481e-02,  1.2354e-01, -2.4380e-02],\n","         [-1.8955e-02,  4.2803e-02,  5.5697e-02, -1.7362e-01, -3.7678e-02,\n","          -9.0903e-02, -1.4517e-02, -5.3633e-02,  1.5707e-02, -9.8046e-02,\n","          -5.6777e-02, -5.9116e-02,  1.0826e-01,  3.6800e-02,  3.6889e-02,\n","          -2.8632e-02, -3.8472e-02,  5.6100e-02,  7.7743e-02,  1.5147e-03],\n","         [ 1.1416e-01,  1.2742e-01, -1.6647e-01,  4.3038e-02, -4.2602e-03,\n","           3.8829e-02,  1.1160e-01, -9.2054e-02, -1.6203e-01,  1.1062e-01,\n","          -9.9848e-02, -6.8622e-02,  2.0462e-02, -6.8610e-02, -1.5922e-01,\n","           3.4190e-03, -7.8148e-02,  5.9786e-02, -5.0608e-02, -6.8845e-02],\n","         [-2.1000e-02,  1.0522e-01,  9.0790e-02, -1.0932e-01,  2.7997e-01,\n","          -3.2578e-02, -1.1524e-01,  8.8823e-02, -3.6167e-02,  2.1537e-01,\n","           8.4741e-02, -1.9872e-02,  1.5753e-01,  8.4912e-02, -1.2289e-01,\n","           8.8839e-02, -5.1649e-02, -8.3326e-03,  1.3105e-02, -8.7910e-02],\n","         [-1.3333e-01,  3.6778e-02, -1.3882e-01, -2.5752e-01, -8.3611e-02,\n","           3.3109e-02, -2.6988e-02,  1.2671e-01,  1.8375e-02, -7.6631e-02,\n","          -4.3958e-02, -1.4365e-01,  1.0858e-01, -1.3811e-01, -9.2041e-02,\n","          -1.6029e-02,  2.3533e-04, -1.5027e-01, -9.0554e-02,  2.6504e-02],\n","         [ 1.1297e-01,  3.4900e-02, -2.5810e-03, -1.5624e-01, -6.1734e-02,\n","           5.2149e-02,  1.0809e-01,  8.8938e-02,  1.3807e-02,  1.2046e-01,\n","           2.8815e-01, -5.9386e-02, -7.6312e-02,  1.5185e-01,  2.3546e-02,\n","           1.1231e-02,  3.9237e-02, -6.5449e-02, -1.0348e-01, -7.7715e-02],\n","         [ 1.2459e-01, -1.4366e-01,  4.9865e-02, -5.5769e-02, -3.5337e-02,\n","           7.4295e-02,  8.4399e-02,  3.4298e-02, -1.8731e-01,  1.5710e-01,\n","           1.3102e-01,  9.1437e-03,  1.0258e-03,  1.8014e-01,  9.4722e-02,\n","          -2.9294e-03, -2.9234e-02, -1.9354e-02,  1.1772e-01,  1.0400e-01],\n","         [-1.6134e-01,  4.6464e-02,  8.6412e-02, -1.5065e-01, -2.9648e-04,\n","          -1.7770e-01,  1.2949e-02, -2.0832e-01, -6.8175e-02, -6.1107e-02,\n","          -7.0885e-02,  1.4515e-01,  5.3551e-02, -3.9957e-02, -9.3308e-02,\n","          -2.3878e-02, -1.0291e-01,  9.7308e-02,  1.9968e-01,  1.0532e-01],\n","         [ 3.3169e-02, -1.6563e-02, -4.0511e-02,  1.7453e-01, -5.7594e-02,\n","           1.5611e-01, -1.1315e-01, -2.9623e-02, -1.7141e-01,  1.5923e-02,\n","          -1.2637e-01,  1.6650e-01,  4.1227e-02,  5.3740e-02,  2.8268e-02,\n","          -1.0925e-01,  1.2412e-02,  1.8371e-01,  8.5549e-04, -1.0170e-01],\n","         [-1.8523e-01, -7.1333e-02, -1.7622e-01,  8.3052e-02,  7.8117e-02,\n","          -8.7568e-02,  6.1398e-02, -5.7645e-02, -4.5614e-03,  3.7196e-02,\n","          -4.4396e-02,  4.1820e-02, -1.6857e-01,  1.1747e-02, -3.4952e-03,\n","          -2.0464e-01, -1.8097e-01, -1.8595e-01,  4.1431e-02,  1.2396e-02],\n","         [ 2.7396e-02, -1.3264e-01,  1.1390e-01,  9.8284e-02, -7.6696e-02,\n","           1.1761e-01, -2.5092e-02, -1.7762e-01, -1.6327e-01,  7.3372e-02,\n","          -1.0405e-02,  8.8122e-02, -8.8373e-03,  2.6767e-02,  2.1235e-01,\n","           1.3968e-01, -4.3283e-02,  3.7497e-02,  4.9445e-02,  7.6139e-02],\n","         [ 7.1009e-03, -4.9353e-02, -3.6228e-04, -4.8029e-02,  1.6834e-01,\n","           1.2407e-01, -2.0362e-02,  4.2829e-02, -1.6546e-02,  1.1932e-01,\n","           1.0489e-01,  5.6861e-02,  8.7126e-02,  6.6057e-02,  1.1741e-01,\n","           5.3113e-02,  1.5190e-02, -5.7723e-02, -1.5718e-01, -2.7848e-03],\n","         [-7.4106e-02,  6.0009e-03,  1.1405e-01,  1.7282e-02, -4.1500e-02,\n","          -8.5313e-02, -1.4301e-01,  1.3328e-01, -1.7767e-01, -9.3478e-02,\n","          -2.3132e-01, -3.1614e-02, -3.4228e-02, -4.0429e-02, -6.3130e-03,\n","          -8.2127e-02, -9.1366e-02,  1.8178e-01, -3.3406e-02,  9.0766e-02],\n","         [-8.3677e-02,  1.6127e-01,  1.5142e-01,  2.3102e-02, -1.0995e-01,\n","           8.7001e-03,  4.7304e-03,  2.3962e-02, -9.7822e-02, -1.5230e-01,\n","           1.6236e-02, -1.0291e-03,  2.0750e-04,  1.0268e-01, -1.4752e-01,\n","           1.0107e-01, -7.4323e-02, -3.9522e-02, -8.2578e-02,  8.9620e-03],\n","         [-1.9058e-01, -5.6809e-02, -5.1576e-02,  1.2639e-01,  1.5070e-02,\n","           6.9552e-02,  5.9388e-04, -1.0489e-01,  9.0720e-02, -8.4544e-02,\n","          -5.2624e-02,  1.8210e-02,  9.4554e-02, -2.0138e-02,  1.5105e-01,\n","          -5.7148e-02,  6.6559e-02,  3.6163e-04,  1.5467e-01,  2.1441e-02]],\n","        device='cuda:0', grad_fn=<CopyBackwards>),\n"," tensor([[-0.1877],\n","         [ 0.1088],\n","         [-0.0082],\n","         [-0.0553],\n","         [ 0.1223],\n","         [-0.0331],\n","         [ 0.0600],\n","         [-0.0768],\n","         [-0.0836],\n","         [ 0.1811],\n","         [-0.0787],\n","         [-0.0585],\n","         [-0.1708],\n","         [ 0.1630],\n","         [ 0.0398],\n","         [ 0.0238],\n","         [ 0.0975],\n","         [-0.1329],\n","         [-0.0541],\n","         [-0.0091],\n","         [-0.1548]], device='cuda:0', grad_fn=<CopyBackwards>)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"1hOFi1DHlx2Q"},"source":["# Loss Function"]},{"cell_type":"code","metadata":{"id":"ttaD9mDElrR1"},"source":["def train_loss(data, target):\n","    out = model(data)\n","    loss = F.mse_loss(out, target)\n","    return loss\n","\n","def test_loss():\n","    loss = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            out = model(data)\n","            loss += F.mse_loss(out, target)\n","            \n","    return loss.item()/n_test_batches\n","\n","\n","def save_start_condition(trainlosslist, testlosslist, timelist):\n","    trainloss = 0.0\n","\n","    for (data, target) in tqdm(train_loader, ncols = 80):\n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        trainloss += loss\n","\n","    timelist.append(0)\n","\n","    testloss = test_loss()\n","\n","    trainlosslist.append(trainloss.item()/n_batches)\n","    testlosslist.append(testloss)\n","    print('Epoch: {}; train loss: {}; test loss: {},  time: {}'.format(0, trainlosslist[-1], testlosslist[-1], np.sum(timelist)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5nub-2OIB1v"},"source":["# SGD"]},{"cell_type":"code","metadata":{"id":"eg5Ei_9Cd80k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616426218908,"user_tz":-300,"elapsed":119742,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"5a6c24b4-9750-4877-b47f-bec99f6a0565"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","Ws = initialize_weights()\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.numel() for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","times = []\n","\n","with torch.no_grad():\n","    save_start_condition(TrainLoss, TestLoss, times)\n","for epoch in range(EPOCHS):\n","    n = 0\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    t0 = time.time()\n","    for (data, target) in train_loader:\n","        \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        trainloss += loss\n","        grads = grad(loss, Ws)  \n","        \n","\n","        with torch.no_grad():\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","            for (W,pG) in zip(Ws, grads):\n","                W -= step_adjust*step_size*pG\n","            kbar.update(n, values=[(\"loss\", loss.item())])    \n","            n = n + 1\n","            \n","    t1 = time.time() - t0\n","    times.append(t1)\n","\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    \n","    \n","#     step_size = 0.01**(1/9)*step_size\n","    testloss = test_loss()\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","\n","    TestLoss.append(testloss)\n","#     print('Epoch: {}; train loss: {}; test loss: {}, train_acc: {}, test_acc:{}, time: {}'\\\n","#      .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'SGD.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:03<00:00, 124.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 10s 22ms/step - loss: 0.1525 - val_loss: 0.1463\n","Epoch: 2/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.1416 - val_loss: 0.1676\n","Epoch: 3/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0859 - val_loss: 0.0421\n","Epoch: 4/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0308 - val_loss: 0.0133\n","Epoch: 5/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0163 - val_loss: 0.0071\n","Epoch: 6/10\n","450/450 [==============================] - 11s 23ms/step - loss: 0.0125 - val_loss: 0.0151\n","Epoch: 7/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0097 - val_loss: 0.0182\n","Epoch: 8/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0076 - val_loss: 0.0138\n","Epoch: 9/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0065 - val_loss: 0.0084\n","Epoch: 10/10\n","450/450 [==============================] - 10s 23ms/step - loss: 0.0062 - val_loss: 0.0040\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dYV3Ro1nl9-w"},"source":["# Adam"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShkQySPkl0jM","executionInfo":{"status":"ok","timestamp":1616426349446,"user_tz":-300,"elapsed":250273,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"b37a688b-f1e5-45bf-9fbc-f93d10979b2d"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","Ws = initialize_weights()\n","step_size = 0.001\n","m0 = [torch.zeros(W.shape).to(device) for W in Ws]\n","v0 = [torch.zeros(W.shape).to(device) for W in Ws]\n","TrainLoss, TestLoss = [], []\n","times = []\n","\n","with torch.no_grad():\n","    save_start_condition(TrainLoss, TestLoss, times)\n","for epoch in range(EPOCHS):\n","    n = 0\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    trainacc = 0.0\n","    t0 = time.time()\n","    for (data, target) in train_loader:\n","        \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        trainloss += loss\n","        grads = grad(loss, Ws, create_graph=True)    \n","\n","        with torch.no_grad():\n","            lmbd = min(n/(n+1), 0.9)\n","            m0 = [lmbd*old + (1.0-lmbd)*new for (old, new) in zip(m0, grads)]\n","            lmbd = min(n/(n+1), 0.999)\n","            v0 = [lmbd*old + (1.0-lmbd)*new*new for (old, new) in zip(v0, grads)]\n","            for (W,m,v) in zip(Ws, m0, v0):\n","                W -= step_size*(m/torch.sqrt(v + 1e-8))\n","            kbar.update(n, values=[(\"loss\", loss.item())])    \n","            n = n + 1\n","            \n","    t1 = time.time() - t0\n","    times.append(t1)\n","\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    \n","    \n","#     step_size = 0.01**(1/9)*step_size\n","    testloss = test_loss()\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","\n","    TestLoss.append(testloss)\n","#     print('Epoch: {}; train loss: {}; test loss: {}, train_acc: {}, test_acc:{}, time: {}'\\\n","#      .format(epoch+1, TrainLoss[-1], TestLoss[-1], TrainAcc[-1], TestAcc[-1],np.sum(times)))\n","\n","scipy.io.savemat(results_dir + 'adam.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:03<00:00, 123.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 12s 27ms/step - loss: 0.0819 - val_loss: 0.0047\n","Epoch: 2/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0103 - val_loss: 0.0015\n","Epoch: 3/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0020 - val_loss: 8.6111e-04\n","Epoch: 4/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0056 - val_loss: 4.6982e-04\n","Epoch: 5/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0035 - val_loss: 4.5371e-04\n","Epoch: 6/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0041 - val_loss: 3.3794e-04\n","Epoch: 7/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0011 - val_loss: 2.5227e-04\n","Epoch: 8/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0018 - val_loss: 2.4613e-04\n","Epoch: 9/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0028 - val_loss: 1.8318e-04\n","Epoch: 10/10\n","450/450 [==============================] - 13s 28ms/step - loss: 0.0027 - val_loss: 1.8486e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fsEdqCMbeK8d"},"source":["# PSGD"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9Mq-i-CJWvm","executionInfo":{"status":"ok","timestamp":1616432340351,"user_tz":-300,"elapsed":353613,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"61eb6304-5a9a-48ab-e160-5cc736f1178a"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","\n","Ws = initialize_weights()\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","# Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","\n","times = []\n","save_start_condition(TrainLoss, TestLoss, times)\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    n = 0\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","\n","        v = [torch.randn(W.shape).to(device) for W in Ws]\n","        Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            pre_grads = [psgd.precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","        kbar.update(n, values=[(\"loss\", loss.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    \n","    testloss = test_loss()\n","\n","    TestLoss.append(testloss)\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","    # step_size = 0.01**(1/9)*step_size\n","    \n","\n","scipy.io.savemat(results_dir + 'Kron.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:05<00:00, 86.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 35s 79ms/step - loss: 0.0611 - val_loss: 0.0020\n","Epoch: 2/10\n","450/450 [==============================] - 35s 77ms/step - loss: 0.0013 - val_loss: 9.2140e-04\n","Epoch: 3/10\n","450/450 [==============================] - 35s 77ms/step - loss: 6.9965e-04 - val_loss: 5.7163e-04\n","Epoch: 4/10\n","450/450 [==============================] - 34s 76ms/step - loss: 4.7069e-04 - val_loss: 4.1109e-04\n","Epoch: 5/10\n","450/450 [==============================] - 35s 77ms/step - loss: 3.5105e-04 - val_loss: 3.1323e-04\n","Epoch: 6/10\n","450/450 [==============================] - 34s 77ms/step - loss: 2.7345e-04 - val_loss: 2.4660e-04\n","Epoch: 7/10\n","450/450 [==============================] - 34s 77ms/step - loss: 2.1767e-04 - val_loss: 1.9946e-04\n","Epoch: 8/10\n","450/450 [==============================] - 34s 76ms/step - loss: 1.7911e-04 - val_loss: 1.6705e-04\n","Epoch: 9/10\n","450/450 [==============================] - 34s 76ms/step - loss: 1.5281e-04 - val_loss: 1.4294e-04\n","Epoch: 10/10\n","450/450 [==============================] - 35s 77ms/step - loss: 1.3212e-04 - val_loss: 1.2575e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zj0sV4GZd80n"},"source":["# DPSGD APPROACH 1"]},{"cell_type":"code","metadata":{"id":"_XPsqTSad80o"},"source":["_tiny = 1.2e-38 \n"," # pi = (torch.trace(Ql)*Qr.shape[0])/(torch.trace(Qr)*Ql.shape[0])\n","    # \n","\n","    \n","def precond_grad_kron(Ql, Qr, Grad):\n","    P1 = Ql.t().mm(Ql)\n","    P2 = Qr.t().mm(Qr)\n","    pi = (torch.trace(P1)*P2.shape[0])/(torch.trace(P2)*P1.shape[0])\n","    IL = torch.ones(P1.shape[0]).to(device)\n","    IR = (torch.ones(P2.shape[0])).to(device)\n","    P1 = P1 + torch.diag(torch.sqrt((pi)*(eta + lambd))*IL)\n","    P2 = P2 + torch.diag(torch.sqrt((1/pi)*(eta + lambd))*IR)\n","\n","    return P1.mm(Grad).mm(P2)\n","\n","def update_lambda(loss1, loss2, M, lambd, omega):\n","    \n","    r = abs(loss2 - loss1)/(M)\n","    # print(r, M, lambd)\n","    if r > 3/4:\n","      lambd = lambd*omega\n","    elif r < 1/4:\n","      lambd = lambd / omega\n","    return lambd\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZdoZajPMBSF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616433410189,"user_tz":-300,"elapsed":228375,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"73f6d3c8-63fb-4749-dd42-173b72b5c1aa"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","\n","Ws = initialize_weights()\n","# Qs = [[0.1*torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, times)\n","\n","lambd = 1\n","update_after = 5\n","omega = (19/20)**update_after\n","\n","eta = 1e-10\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    n = 0\n","    t0 = time.time()\n","    \n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        if n % 1 == 0:\n","          v = [torch.randn(W.shape).to(device) for W in Ws]\n","          Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            pre_grads = [precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","            if n % update_after == 0 and lambd > 1e-10:\n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(g*pg) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(-step_size * pg*psgd.precond_grad_kron(q[0], q[1], -step_size * pg)) \\\n","                #              for (g, pg, q) in zip(grads, pre_grads, Qs)])\n","                loss2 = F.mse_loss(model(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M,  lambd, omega)\n","\n","        kbar.update(n, values=[(\"loss\", loss.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","\n","    testloss = test_loss()\n","\n","    TestLoss.append(testloss)\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","    # step_size = 0.01**(1/9)*step_size\n","    \n","\n","scipy.io.savemat(results_dir + 'Kron_damped.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:05<00:00, 87.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 35s 79ms/step - loss: 0.0576 - val_loss: 0.0024\n","Epoch: 2/10\n","450/450 [==============================] - 35s 78ms/step - loss: 0.0015 - val_loss: 9.9874e-04\n","Epoch: 3/10\n","450/450 [==============================] - 35s 77ms/step - loss: 7.6693e-04 - val_loss: 6.2921e-04\n","Epoch: 4/10\n","450/450 [==============================] - 35s 77ms/step - loss: 5.3785e-04 - val_loss: 4.7327e-04\n","Epoch: 5/10\n","450/450 [==============================] - 35s 78ms/step - loss: 4.3112e-04 - val_loss: 3.9303e-04\n","Epoch: 6/10\n","450/450 [==============================] - 35s 78ms/step - loss: 3.6721e-04 - val_loss: 3.4055e-04\n","Epoch: 7/10\n","450/450 [==============================] - 35s 78ms/step - loss: 3.2185e-04 - val_loss: 3.0138e-04\n","Epoch: 8/10\n","450/450 [==============================] - 35s 78ms/step - loss: 2.8482e-04 - val_loss: 2.6462e-04\n","Epoch: 9/10\n","450/450 [==============================] - 35s 78ms/step - loss: 2.5557e-04 - val_loss: 2.3676e-04\n","Epoch: 10/10\n","450/450 [==============================] - 35s 78ms/step - loss: 2.3070e-04 - val_loss: 2.1525e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"99nKUMsZd80p"},"source":["# DPSGD APPROACH 2"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHxIW0_v6rfw","executionInfo":{"status":"ok","timestamp":1616432691665,"user_tz":-300,"elapsed":351281,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"76f61aa9-342d-4817-f78b-71239c8993ce"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","\n","Ws = initialize_weights()\n","# Qs = [[0.1*torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","dQs = [[torch.ones(W.shape[0],1).to(device), torch.ones(1,W.shape[1]).to(device)] for W in Ws]\n","\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, times)\n","\n","lambd = 1\n","update_after = 5\n","omega = (19/20)**update_after\n","# \n","eta = 1e-10\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    n = 0\n","    t0 = time.time()\n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        if n % 1 == 0:\n","          v = [torch.randn(W.shape).to(device) for W in Ws]\n","          Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            pre_grads = [psgd.precond_grad_kron(q[0], q[1], g) for (q, g) in zip(Qs, grads)]\n","            damp_grads = [((lambd+eta)**0.5)*g for g in grads]\n","            pre_grads = [pg+dg for (pg, dg) in zip(pre_grads, damp_grads)] \n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","            if n % update_after == 0 and lambd > 1e-10:\n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(g*pg) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(-step_size * pg*psgd.precond_grad_kron(q[0], q[1], -step_size * pg)) \\\n","                #              for (g, pg, q) in zip(grads, pre_grads, Qs)])\n","                loss2 = F.mse_loss(model(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M,  lambd, omega)\n","\n","        kbar.update(n, values=[(\"loss\", loss.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","\n","    testloss = test_loss()\n","\n","    TestLoss.append(testloss)\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","    # step_size = 0.01**(1/9)*step_size\n","    \n","\n","scipy.io.savemat(results_dir + 'Kron_damped2.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:05<00:00, 86.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 35s 77ms/step - loss: 0.0675 - val_loss: 0.0022\n","Epoch: 2/10\n","450/450 [==============================] - 34s 76ms/step - loss: 0.0014 - val_loss: 9.9354e-04\n","Epoch: 3/10\n","450/450 [==============================] - 34s 75ms/step - loss: 8.0202e-04 - val_loss: 6.3238e-04\n","Epoch: 4/10\n","450/450 [==============================] - 34s 76ms/step - loss: 5.3391e-04 - val_loss: 4.3072e-04\n","Epoch: 5/10\n","450/450 [==============================] - 34s 76ms/step - loss: 3.7209e-04 - val_loss: 3.0525e-04\n","Epoch: 6/10\n","450/450 [==============================] - 34s 76ms/step - loss: 2.6700e-04 - val_loss: 2.2828e-04\n","Epoch: 7/10\n","450/450 [==============================] - 35s 77ms/step - loss: 2.0267e-04 - val_loss: 1.8017e-04\n","Epoch: 8/10\n","450/450 [==============================] - 34s 76ms/step - loss: 1.6261e-04 - val_loss: 1.4853e-04\n","Epoch: 9/10\n","450/450 [==============================] - 35s 77ms/step - loss: 1.3648e-04 - val_loss: 1.2874e-04\n","Epoch: 10/10\n","450/450 [==============================] - 34s 76ms/step - loss: 1.1794e-04 - val_loss: 1.1193e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wwfjKXg7NsoN"},"source":["# DPSGD-M"]},{"cell_type":"code","metadata":{"id":"oQx4Gvt279m7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616433050733,"user_tz":-300,"elapsed":710336,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"74477e11-e9f9-428c-beb7-b86965fe7cb3"},"source":["def precond_kron(Ql, Qr, Pl, Pr, beta):\n","    P1 = Ql.t().mm(Ql)\n","    P2 = Qr.t().mm(Qr)\n","    pi = (torch.trace(P1)*P2.shape[0])/(torch.trace(P2)*P1.shape[0])\n","    IL = torch.ones(P1.shape[0]).to(device)\n","    IR = (torch.ones(P2.shape[0])).to(device)\n","    P1 = P1 + torch.diag(torch.sqrt((pi)*(eta + lambd))*IL)\n","    P2 = P2 + torch.diag(torch.sqrt((1/pi)*(eta + lambd))*IR)\n","\n","    Pl = beta*Pl + (1-beta)*P1 \n","    Pr = beta*Pr + (1-beta)*P2 \n","\n","    return [P1, P2, Pl, Pr]\n","\n","def precond_kron2(Ql, Qr, Pl, Pr, beta):\n","    P1 = Ql.t().mm(Ql)\n","    P2 = Qr.t().mm(Qr)\n","    Pl = beta*Pl + (1-beta)*P1 \n","    Pr = beta*Pr + (1-beta)*P2 \n","    return [P1, P2, Pl, Pr]\n","\n","def precond_grad_kron2(Pl, Pr, Grad):\n","    return Pl.mm(Grad).mm(Pr)\n","\n","torch.manual_seed(1)\n","np.random.seed(0)\n","\n","Ws = initialize_weights()\n","# Qs = [[0.1*torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","Ps = [[torch.zeros(W.shape[0]).to(device), torch.zeros(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, times)\n","\n","lambd = 1\n","update_after = 5\n","omega = (19/20)**update_after\n","eta = 1e-10\n","beta = 0.7\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    n = 0\n","    trainloss = 0.0\n","    t0 = time.time()\n","   \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss= train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","\n","        v = [torch.randn(W.shape).to(device) for W in Ws]\n","        Hv = grad(grads, Ws, v)\n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            beta = min(n/(n+1), 0.7)\n","            Ps = [precond_kron(q[0], q[1], p[0], p[1], beta) for (q, p) in zip(Qs, Ps)]\n","            pre_grads = [precond_grad_kron2(p[2], p[3], g) for (p, g) in zip(Ps, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","            if n % update_after == 0 and lambd > 1e-10:\n","              \n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                loss2 = F.mse_loss(model(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M, lambd, omega)\n","        kbar.update(n, values=[(\"loss\", loss.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","\n","    testloss = test_loss()\n","\n","    TestLoss.append(testloss)\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","    # step_size = 0.01**(1/9)*step_size\n","    \n","\n","scipy.io.savemat(results_dir + 'mod_psgd1.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:05<00:00, 88.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 36s 80ms/step - loss: 0.0601 - val_loss: 0.0024\n","Epoch: 2/10\n","450/450 [==============================] - 35s 78ms/step - loss: 0.0015 - val_loss: 0.0011\n","Epoch: 3/10\n","450/450 [==============================] - 35s 78ms/step - loss: 7.8749e-04 - val_loss: 6.5568e-04\n","Epoch: 4/10\n","450/450 [==============================] - 35s 78ms/step - loss: 5.4210e-04 - val_loss: 4.9084e-04\n","Epoch: 5/10\n","450/450 [==============================] - 35s 78ms/step - loss: 4.2312e-04 - val_loss: 3.8847e-04\n","Epoch: 6/10\n","450/450 [==============================] - 35s 78ms/step - loss: 3.5095e-04 - val_loss: 3.2957e-04\n","Epoch: 7/10\n","450/450 [==============================] - 35s 78ms/step - loss: 3.0060e-04 - val_loss: 2.8617e-04\n","Epoch: 8/10\n","450/450 [==============================] - 35s 78ms/step - loss: 2.6151e-04 - val_loss: 2.4687e-04\n","Epoch: 9/10\n","450/450 [==============================] - 35s 78ms/step - loss: 2.3173e-04 - val_loss: 2.1945e-04\n","Epoch: 10/10\n","450/450 [==============================] - 35s 78ms/step - loss: 2.0714e-04 - val_loss: 1.9685e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dt6G0rbf6_O1"},"source":["# dpsgd-m 2"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVCIdUxV7C9k","executionInfo":{"status":"ok","timestamp":1616431780504,"user_tz":-300,"elapsed":350847,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"5759bdbd-bb57-4ef6-9273-7175fe47a0e6"},"source":["\n","torch.manual_seed(1)\n","np.random.seed(0)\n","\n","Ws = initialize_weights()\n","Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","\n","# Qs = [[torch.eye(W.shape[0]).to(device), torch.eye(W.shape[1]).to(device)] for W in Ws]\n","dQs = [[torch.ones(W.shape[0],1).to(device), torch.ones(1,W.shape[1]).to(device)] for W in Ws]\n","\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, times)\n","\n","lambd = 1\n","update_after = 10\n","omega = (19/20)**update_after\n","beta = 0.7\n","eta = 1e-10\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    n = 0\n","    t0 = time.time()\n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","      \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        if n % 1 == 0:\n","          v = [torch.randn(W.shape).to(device) for W in Ws]\n","          Hv = grad(grads, Ws, v)\n","        \n","        with torch.no_grad():\n","            Qs = [psgd.update_precond_kron(q[0], q[1], dw, dg) for (q, dw, dg) in zip(Qs, v, Hv)]\n","            beta = min(n/(n+1), 0.7)\n","            Ps = [precond_kron2(q[0], q[1], p[0], p[1], beta) for (q, p) in zip(Qs, Ps)]\n","            pre_grads = [precond_grad_kron2(p[2], p[3], g) for (p, g) in zip(Ps, grads)]\n","            \n","            damp_grads = [((lambd+eta)**0.5)*g for g in grads]\n","            pre_grads = [pg+dg for (pg, dg) in zip(pre_grads, damp_grads)] \n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","            if n % update_after == 0 and lambd > 1e-10:\n","                M = min([0.5*torch.dot(g.view(-1,), step_size*pg.view(-1,)) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(g*pg) for (g, pg) in zip(grads, pre_grads)])\n","                # M = 0.5*sum([torch.sum(-step_size * pg*psgd.precond_grad_kron(q[0], q[1], -step_size * pg)) \\\n","                #              for (g, pg, q) in zip(grads, pre_grads, Qs)])\n","                loss2 = F.mse_loss(model(data), target)\n","                loss1 = loss\n","                lambd = update_lambda(loss1, loss2, M,  lambd, omega)\n","\n","        kbar.update(n, values=[(\"loss\", loss.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","\n","    testloss = test_loss()\n","\n","    TestLoss.append(testloss)\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","    # step_size = 0.01**(1/9)*step_size\n","    \n","\n","scipy.io.savemat(results_dir + 'mod_psgd2.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:05<00:00, 87.00it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 34s 76ms/step - loss: 0.0654 - val_loss: 0.0027\n","Epoch: 2/10\n","450/450 [==============================] - 34s 76ms/step - loss: 0.0017 - val_loss: 0.0012\n","Epoch: 3/10\n","450/450 [==============================] - 34s 76ms/step - loss: 8.8396e-04 - val_loss: 6.5517e-04\n","Epoch: 4/10\n","450/450 [==============================] - 34s 76ms/step - loss: 5.2386e-04 - val_loss: 4.1191e-04\n","Epoch: 5/10\n","450/450 [==============================] - 34s 76ms/step - loss: 3.5349e-04 - val_loss: 2.8831e-04\n","Epoch: 6/10\n","450/450 [==============================] - 34s 76ms/step - loss: 2.5486e-04 - val_loss: 2.1568e-04\n","Epoch: 7/10\n","450/450 [==============================] - 34s 76ms/step - loss: 1.9678e-04 - val_loss: 1.7175e-04\n","Epoch: 8/10\n","450/450 [==============================] - 35s 77ms/step - loss: 1.5988e-04 - val_loss: 1.4320e-04\n","Epoch: 9/10\n","450/450 [==============================] - 34s 77ms/step - loss: 1.3532e-04 - val_loss: 1.2442e-04\n","Epoch: 10/10\n","450/450 [==============================] - 34s 76ms/step - loss: 1.1780e-04 - val_loss: 1.1053e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VpQeroRDd80p"},"source":["# Shampoo"]},{"cell_type":"code","metadata":{"id":"ZQ-DmeXxd80p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616428220482,"user_tz":-300,"elapsed":146271,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"ddd64074-568c-4adf-863c-a870d3320eda"},"source":["torch.manual_seed(1)\n","np.random.seed(0)\n","\n","Ws = initialize_weights()\n","Qs = [[0.01*torch.eye(W.shape[0]).to(device), 0.01*torch.eye(W.shape[1]).to(device)] for W in Ws]\n","step_size = 0.1\n","grad_norm_clip_thr = 0.1*sum(W.shape[0]*W.shape[1] for W in Ws)**0.5\n","TrainLoss, TestLoss = [], []\n","times = []\n","save_start_condition(TrainLoss, TestLoss, times)\n","\n","def matrix_power(matrix, power):\n","    # use CPU for svd for speed up\n","    matrix = matrix.cpu()\n","    u, s, v = torch.svd(matrix)\n","    return (u @ s.pow_(power).diag() @ v.t()).cuda()\n","\n","for epoch in range(EPOCHS):\n","    kbar = pkbar.Kbar(target=n_batches, epoch=epoch, num_epochs=EPOCHS, width=30, always_stateful=False, interval = 1)\n","    trainloss = 0.0\n","    n = 0\n","    t0 = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        \n","      \n","        data, target = data.to(device), target.to(device)\n","        loss = train_loss(data, target)\n","        \n","        grads = grad(loss, Ws, create_graph=True)\n","        \n","        trainloss += loss\n","        \n","        with torch.no_grad():\n","            Qs = [[q[0] + g.mm(g.t()), q[1] + (g.t()).mm(g)] for (q, g) in zip(Qs, grads)]\n","            inv_Qs = [[matrix_power(q[0], -1/4), matrix_power(q[1], -1/4)]for q in Qs]\n","            pre_grads = [q[0].mm(g).mm(q[1]) for (q, g) in zip(inv_Qs, grads)]\n","            grad_norm = torch.sqrt(sum([torch.sum(g*g) for g in pre_grads]))\n","            step_adjust = min(grad_norm_clip_thr/(grad_norm + 1.2e-38), 1.0)\n","            for i in range(len(Ws)):\n","                Ws[i] -= step_adjust*step_size*pre_grads[i]\n","\n","        kbar.update(n, values=[(\"loss\", loss.item())])\n","        n += 1\n","\n","    t1 = time.time() - t0\n","    times.append(t1)\n","    TrainLoss.append(trainloss.item()/n_batches)\n","    \n","    testloss = test_loss()\n","\n","    TestLoss.append(testloss)\n","    kbar.add(1, values=[(\"val_loss\", testloss)])\n","    # step_size = 0.1**(1/9)*step_size\n","   \n","\n","scipy.io.savemat(results_dir + 'shampoo.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'Time':times})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|                                                   | 0/450 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","100%|| 450/450 [00:05<00:00, 89.22it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0; train loss: 0.20708046807183159; test loss: 0.20636802673339844,  time: 0\n","Epoch: 1/10\n","450/450 [==============================] - 14s 31ms/step - loss: 0.0587 - val_loss: 0.0049\n","Epoch: 2/10\n","450/450 [==============================] - 14s 31ms/step - loss: 0.0028 - val_loss: 0.0019\n","Epoch: 3/10\n","450/450 [==============================] - 14s 32ms/step - loss: 0.0016 - val_loss: 0.0013\n","Epoch: 4/10\n","450/450 [==============================] - 14s 31ms/step - loss: 0.0012 - val_loss: 0.0012\n","Epoch: 5/10\n","450/450 [==============================] - 14s 31ms/step - loss: 8.5416e-04 - val_loss: 7.2847e-04\n","Epoch: 6/10\n","450/450 [==============================] - 14s 31ms/step - loss: 7.0286e-04 - val_loss: 5.9225e-04\n","Epoch: 7/10\n","450/450 [==============================] - 14s 31ms/step - loss: 5.3682e-04 - val_loss: 3.5852e-04\n","Epoch: 8/10\n","450/450 [==============================] - 14s 31ms/step - loss: 5.1949e-04 - val_loss: 6.3872e-04\n","Epoch: 9/10\n","450/450 [==============================] - 14s 31ms/step - loss: 3.9805e-04 - val_loss: 5.5933e-04\n","Epoch: 10/10\n","450/450 [==============================] - 14s 31ms/step - loss: 4.1539e-04 - val_loss: 2.8332e-04\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VKqHDHWs7rPo"},"source":["# Comparison"]},{"cell_type":"code","metadata":{"id":"2sNOWNfay0sr"},"source":["# opts = ['adam','Kron','SCAN','SCAW', 'fisher_kron','fisher_SCAN','fisher_SCAW','Kron_test','Kron_test2','Kron_damped','fisher_kron_damped','KFAC', 'shampoo']\n","opts = ['SGD','adam','Kron','Kron_damped2','mod_psgd1', 'shampoo']\n","\n","total_train_time = {}\n","opts_data = {}\n","times = {}\n","train_times = {}\n","test_times = {}\n","train_losses = {}\n","test_losses = {}\n","train_accs = {}\n","test_accs = {}\n","\n","\n","for opt in opts:\n","\topts_data[opt] = scipy.io.loadmat(results_dir+opt+'.mat')\t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gIYMFyL-C8d"},"source":["colors = ['#0000FF','#00FF00','#FF0000','#33F0FF','#FFA833','#FFF933','#000000','#33E0FF', '#FF33E6','#D433FF','#888A0B','#8A0B1E','#B498DF','#1B786D']\n","# colors = ['#0000FF','#00FF00','#FF0000','#33F0FF','#FFA833','#FFF933','#000000','#33E0FF','#FF33E6','#D433FF','#888A0B','#8A0B1E','#B498DF','#1B786D']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Rez_eVv8XdP"},"source":["for opt in opts:\n","    # print(opt)\n","    data = opts_data[opt]\n","    times[opt] = data.get('Time')\n","    train_times[opt] = np.cumsum(times[opt])\n","    test_times[opt] = np.cumsum(times[opt])\n","    total_train_time[opt] = np.sum(times[opt])\n","    train_losses[opt] = data.get('TrainLoss').reshape(EPOCHS+1,)\n","    test_losses[opt] = data.get('TestLoss').reshape(EPOCHS+1,)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pv6Ln4X5d80t","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1616431271815,"user_tz":-300,"elapsed":2163,"user":{"displayName":"Muhammad Usman Qadeer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgElGh2_2-f9BDHTCwkWd5JW2fmnV8XfiuLlkBR=s64","userId":"08373743905078334901"}},"outputId":"8ec367a0-ce4b-461c-8113-05d3d03f7222"},"source":["# plot train_losses vs Iterations\n","plot_loss_metrics(None,train_losses,'Train Loss vs EPOCHS', 'EPOCHS','Train Loss')\n","# plot test_losses vs Iterations\n","plot_loss_metrics(None,test_losses,'Test Loss vs EPOCHS', 'EPOCHS','Test Loss')\n","# # plot test_losses vs Iterations\n","plot_loss_metrics(train_times,train_losses,'Train Loss vs Time', 'Time','Train Loss')\n","# plot test_losses vs Iterations\n","plot_loss_metrics(test_times,test_losses,'Test Loss vs Time', 'Time','Test Loss')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"9208e1ae-1d9b-4a24-84a1-8ecde41268d6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"9208e1ae-1d9b-4a24-84a1-8ecde41268d6\")) {\n","                    Plotly.newPlot(\n","                        '9208e1ae-1d9b-4a24-84a1-8ecde41268d6',\n","                        [{\"line\": {\"color\": \"#0000FF\"}, \"mode\": \"lines\", \"name\": \"SGD\", \"type\": \"scatter\", \"y\": [0.20708046807183159, 0.15256895277235244, 0.14165208604600693, 0.08600506252712674, 0.03085193634033203, 0.016284848319159613, 0.012520410749647352, 0.009738377465142145, 0.007612146271599664, 0.006507706642150879, 0.006252932548522949]}, {\"line\": {\"color\": \"#00FF00\"}, \"mode\": \"lines\", \"name\": \"adam\", \"type\": \"scatter\", \"y\": [0.20708046807183159, 0.08214564005533855, 0.010337550905015733, 0.0020320439338684083, 0.005570584932963053, 0.0035091363059149847, 0.004067059622870552, 0.0010693292485343085, 0.001749706268310547, 0.0028160209125942656, 0.00273296939002143]}, {\"line\": {\"color\": \"#FF0000\"}, \"mode\": \"lines\", \"name\": \"Kron\", \"type\": \"scatter\", \"y\": [0.20708046807183159, 0.12358262803819445, 0.00232969601949056, 0.0011103961865107218, 0.0006620286570654975, 0.0004635864496231079, 0.0003563434547848172, 0.00028650157981448705, 0.00023860704567697314, 0.00020526145895322163, 0.0001789611412419213]}, {\"line\": {\"color\": \"#33F0FF\"}, \"mode\": \"lines\", \"name\": \"Kron_damped2\", \"type\": \"scatter\", \"y\": [0.20708046807183159, 0.11418849521213108, 0.002510788705613878, 0.0011834918128119576, 0.0007875415351655748, 0.000595673123995463, 0.0004749585191408793, 0.00038592156436708237, 0.00032219055626127454, 0.00027515287200609843, 0.00023791737026638456]}, {\"line\": {\"color\": \"#FFA833\"}, \"mode\": \"lines\", \"name\": \"mod_psgd1\", \"type\": \"scatter\", \"y\": [0.20708046807183159, 0.11812958611382378, 0.0024610326025221083, 0.0011237902111477323, 0.0007306830088297526, 0.0005413024624188741, 0.0004253801041179233, 0.00034305012888378566, 0.00028603639867570663, 0.00024494810236824885, 0.00021284431219100952]}, {\"line\": {\"color\": \"#FFF933\"}, \"mode\": \"lines\", \"name\": \"shampoo\", \"type\": \"scatter\", \"y\": [0.20708046807183159, 0.0589478513929579, 0.0027686712476942274, 0.0015504617161220975, 0.0011635871728261312, 0.0008551272418763902, 0.0007026145193311903, 0.0005369953645600212, 0.0005193926228417291, 0.0003986920250786675, 0.000415827731291453]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Train Loss vs EPOCHS\"}, \"xaxis\": {\"title\": {\"text\": \"EPOCHS\"}}, \"yaxis\": {\"title\": {\"text\": \"Train Loss\"}, \"type\": \"log\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('9208e1ae-1d9b-4a24-84a1-8ecde41268d6');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"532e4c98-55fa-46eb-92e5-b675d0f5fad3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"532e4c98-55fa-46eb-92e5-b675d0f5fad3\")) {\n","                    Plotly.newPlot(\n","                        '532e4c98-55fa-46eb-92e5-b675d0f5fad3',\n","                        [{\"line\": {\"color\": \"#0000FF\"}, \"mode\": \"lines\", \"name\": \"SGD\", \"type\": \"scatter\", \"y\": [0.20636802673339844, 0.14634658813476562, 0.16764894485473633, 0.042076611518859865, 0.013347675800323486, 0.007145305275917053, 0.015121037960052491, 0.018176056146621704, 0.013844256401062011, 0.00836294710636139, 0.00401199609041214]}, {\"line\": {\"color\": \"#00FF00\"}, \"mode\": \"lines\", \"name\": \"adam\", \"type\": \"scatter\", \"y\": [0.20636802673339844, 0.004722580313682556, 0.00153205007314682, 0.0008611063659191132, 0.0004698183760046959, 0.0004537088423967361, 0.0003379381448030472, 0.00025227108970284464, 0.00024613238871097566, 0.00018318120390176774, 0.00018486037850379944]}, {\"line\": {\"color\": \"#FF0000\"}, \"mode\": \"lines\", \"name\": \"Kron\", \"type\": \"scatter\", \"y\": [0.20636802673339844, 0.004276225864887237, 0.001489628255367279, 0.0008430615812540054, 0.0005518833547830582, 0.00040882721543312073, 0.0003204989805817604, 0.0002647550590336323, 0.0002223924547433853, 0.00019202848896384238, 0.0001671379618346691]}, {\"line\": {\"color\": \"#33F0FF\"}, \"mode\": \"lines\", \"name\": \"Kron_damped2\", \"type\": \"scatter\", \"y\": [0.20636802673339844, 0.004979139268398285, 0.001580238938331604, 0.0009650560468435288, 0.000701025277376175, 0.0005507414042949676, 0.00044614531099796296, 0.0003647889196872711, 0.0003083227202296257, 0.000264024306088686, 0.00022777551785111428]}, {\"line\": {\"color\": \"#FFA833\"}, \"mode\": \"lines\", \"name\": \"mod_psgd1\", \"type\": \"scatter\", \"y\": [0.20636802673339844, 0.005042309761047363, 0.0015141578018665314, 0.0009064263105392456, 0.0006400838494300843, 0.0004929053410887718, 0.0003944946080446243, 0.00032041322439908983, 0.00027107685804367065, 0.00023298732936382293, 0.0002024727314710617]}, {\"line\": {\"color\": \"#FFF933\"}, \"mode\": \"lines\", \"name\": \"shampoo\", \"type\": \"scatter\", \"y\": [0.20636802673339844, 0.004884045422077179, 0.0018698339164257049, 0.0012517178058624268, 0.0012238848209381104, 0.0007284671813249588, 0.0005922457948327064, 0.0003585176542401314, 0.0006387247890233994, 0.0005593283474445343, 0.0002833213657140732]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Test Loss vs EPOCHS\"}, \"xaxis\": {\"title\": {\"text\": \"EPOCHS\"}}, \"yaxis\": {\"title\": {\"text\": \"Test Loss\"}, \"type\": \"log\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('532e4c98-55fa-46eb-92e5-b675d0f5fad3');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"26a856c6-0d0c-41cb-a2cc-0ad63ede41d6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"26a856c6-0d0c-41cb-a2cc-0ad63ede41d6\")) {\n","                    Plotly.newPlot(\n","                        '26a856c6-0d0c-41cb-a2cc-0ad63ede41d6',\n","                        [{\"line\": {\"color\": \"#0000FF\"}, \"mode\": \"lines\", \"name\": \"SGD\", \"type\": \"scatter\", \"x\": [0.0, 9.345929622650146, 19.060466051101685, 28.736385345458984, 38.378926038742065, 48.088231325149536, 57.83703565597534, 67.44496822357178, 77.08974647521973, 86.68485188484192, 96.28718686103821], \"y\": [0.20708046807183159, 0.15256895277235244, 0.14165208604600693, 0.08600506252712674, 0.03085193634033203, 0.016284848319159613, 0.012520410749647352, 0.009738377465142145, 0.007612146271599664, 0.006507706642150879, 0.006252932548522949]}, {\"line\": {\"color\": \"#00FF00\"}, \"mode\": \"lines\", \"name\": \"adam\", \"type\": \"scatter\", \"x\": [0.0, 11.575199604034424, 23.250121593475342, 34.96106815338135, 46.573086977005005, 58.28660774230957, 69.82088541984558, 81.43586468696594, 93.04944467544556, 104.67068028450012, 116.4135730266571], \"y\": [0.20708046807183159, 0.08214564005533855, 0.010337550905015733, 0.0020320439338684083, 0.005570584932963053, 0.0035091363059149847, 0.004067059622870552, 0.0010693292485343085, 0.001749706268310547, 0.0028160209125942656, 0.00273296939002143]}, {\"line\": {\"color\": \"#FF0000\"}, \"mode\": \"lines\", \"name\": \"Kron\", \"type\": \"scatter\", \"x\": [0.0, 32.34613227844238, 64.88914632797241, 97.55649328231812, 130.24189257621765, 162.798485994339, 195.3666124343872, 228.1405544281006, 260.5552442073822, 293.14576625823975, 325.6194643974304], \"y\": [0.20708046807183159, 0.12358262803819445, 0.00232969601949056, 0.0011103961865107218, 0.0006620286570654975, 0.0004635864496231079, 0.0003563434547848172, 0.00028650157981448705, 0.00023860704567697314, 0.00020526145895322163, 0.0001789611412419213]}, {\"line\": {\"color\": \"#33F0FF\"}, \"mode\": \"lines\", \"name\": \"Kron_damped2\", \"type\": \"scatter\", \"x\": [0.0, 33.64076638221741, 66.46502423286438, 99.19425845146179, 131.7985692024231, 164.44884872436523, 197.2433624267578, 229.8982710838318, 262.65540504455566, 295.39354944229126, 328.26861357688904], \"y\": [0.20708046807183159, 0.11418849521213108, 0.002510788705613878, 0.0011834918128119576, 0.0007875415351655748, 0.000595673123995463, 0.0004749585191408793, 0.00038592156436708237, 0.00032219055626127454, 0.00027515287200609843, 0.00023791737026638456]}, {\"line\": {\"color\": \"#FFA833\"}, \"mode\": \"lines\", \"name\": \"mod_psgd1\", \"type\": \"scatter\", \"x\": [0.0, 33.90521049499512, 67.22459936141968, 100.74203276634216, 134.12694430351257, 167.55490851402283, 200.96612429618835, 234.39079761505127, 267.824583530426, 301.29397773742676, 334.9088485240936], \"y\": [0.20708046807183159, 0.11812958611382378, 0.0024610326025221083, 0.0011237902111477323, 0.0007306830088297526, 0.0005413024624188741, 0.0004253801041179233, 0.00034305012888378566, 0.00028603639867570663, 0.00024494810236824885, 0.00021284431219100952]}, {\"line\": {\"color\": \"#FFF933\"}, \"mode\": \"lines\", \"name\": \"shampoo\", \"type\": \"scatter\", \"x\": [0.0, 12.96326994895935, 25.840397596359253, 39.02193474769592, 51.9074821472168, 64.82911610603333, 77.7620005607605, 90.708571434021, 103.80378746986389, 116.79670238494873, 129.6767499446869], \"y\": [0.20708046807183159, 0.0589478513929579, 0.0027686712476942274, 0.0015504617161220975, 0.0011635871728261312, 0.0008551272418763902, 0.0007026145193311903, 0.0005369953645600212, 0.0005193926228417291, 0.0003986920250786675, 0.000415827731291453]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Train Loss vs Time\"}, \"xaxis\": {\"title\": {\"text\": \"Time\"}}, \"yaxis\": {\"title\": {\"text\": \"Train Loss\"}, \"type\": \"log\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('26a856c6-0d0c-41cb-a2cc-0ad63ede41d6');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"41b1edef-c61e-4657-946f-1dfb08e6083a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"41b1edef-c61e-4657-946f-1dfb08e6083a\")) {\n","                    Plotly.newPlot(\n","                        '41b1edef-c61e-4657-946f-1dfb08e6083a',\n","                        [{\"line\": {\"color\": \"#0000FF\"}, \"mode\": \"lines\", \"name\": \"SGD\", \"type\": \"scatter\", \"x\": [0.0, 9.345929622650146, 19.060466051101685, 28.736385345458984, 38.378926038742065, 48.088231325149536, 57.83703565597534, 67.44496822357178, 77.08974647521973, 86.68485188484192, 96.28718686103821], \"y\": [0.20636802673339844, 0.14634658813476562, 0.16764894485473633, 0.042076611518859865, 0.013347675800323486, 0.007145305275917053, 0.015121037960052491, 0.018176056146621704, 0.013844256401062011, 0.00836294710636139, 0.00401199609041214]}, {\"line\": {\"color\": \"#00FF00\"}, \"mode\": \"lines\", \"name\": \"adam\", \"type\": \"scatter\", \"x\": [0.0, 11.575199604034424, 23.250121593475342, 34.96106815338135, 46.573086977005005, 58.28660774230957, 69.82088541984558, 81.43586468696594, 93.04944467544556, 104.67068028450012, 116.4135730266571], \"y\": [0.20636802673339844, 0.004722580313682556, 0.00153205007314682, 0.0008611063659191132, 0.0004698183760046959, 0.0004537088423967361, 0.0003379381448030472, 0.00025227108970284464, 0.00024613238871097566, 0.00018318120390176774, 0.00018486037850379944]}, {\"line\": {\"color\": \"#FF0000\"}, \"mode\": \"lines\", \"name\": \"Kron\", \"type\": \"scatter\", \"x\": [0.0, 32.34613227844238, 64.88914632797241, 97.55649328231812, 130.24189257621765, 162.798485994339, 195.3666124343872, 228.1405544281006, 260.5552442073822, 293.14576625823975, 325.6194643974304], \"y\": [0.20636802673339844, 0.004276225864887237, 0.001489628255367279, 0.0008430615812540054, 0.0005518833547830582, 0.00040882721543312073, 0.0003204989805817604, 0.0002647550590336323, 0.0002223924547433853, 0.00019202848896384238, 0.0001671379618346691]}, {\"line\": {\"color\": \"#33F0FF\"}, \"mode\": \"lines\", \"name\": \"Kron_damped2\", \"type\": \"scatter\", \"x\": [0.0, 33.64076638221741, 66.46502423286438, 99.19425845146179, 131.7985692024231, 164.44884872436523, 197.2433624267578, 229.8982710838318, 262.65540504455566, 295.39354944229126, 328.26861357688904], \"y\": [0.20636802673339844, 0.004979139268398285, 0.001580238938331604, 0.0009650560468435288, 0.000701025277376175, 0.0005507414042949676, 0.00044614531099796296, 0.0003647889196872711, 0.0003083227202296257, 0.000264024306088686, 0.00022777551785111428]}, {\"line\": {\"color\": \"#FFA833\"}, \"mode\": \"lines\", \"name\": \"mod_psgd1\", \"type\": \"scatter\", \"x\": [0.0, 33.90521049499512, 67.22459936141968, 100.74203276634216, 134.12694430351257, 167.55490851402283, 200.96612429618835, 234.39079761505127, 267.824583530426, 301.29397773742676, 334.9088485240936], \"y\": [0.20636802673339844, 0.005042309761047363, 0.0015141578018665314, 0.0009064263105392456, 0.0006400838494300843, 0.0004929053410887718, 0.0003944946080446243, 0.00032041322439908983, 0.00027107685804367065, 0.00023298732936382293, 0.0002024727314710617]}, {\"line\": {\"color\": \"#FFF933\"}, \"mode\": \"lines\", \"name\": \"shampoo\", \"type\": \"scatter\", \"x\": [0.0, 12.96326994895935, 25.840397596359253, 39.02193474769592, 51.9074821472168, 64.82911610603333, 77.7620005607605, 90.708571434021, 103.80378746986389, 116.79670238494873, 129.6767499446869], \"y\": [0.20636802673339844, 0.004884045422077179, 0.0018698339164257049, 0.0012517178058624268, 0.0012238848209381104, 0.0007284671813249588, 0.0005922457948327064, 0.0003585176542401314, 0.0006387247890233994, 0.0005593283474445343, 0.0002833213657140732]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Test Loss vs Time\"}, \"xaxis\": {\"title\": {\"text\": \"Time\"}}, \"yaxis\": {\"title\": {\"text\": \"Test Loss\"}, \"type\": \"log\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('41b1edef-c61e-4657-946f-1dfb08e6083a');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"y8z5TPf6fT2A"},"source":[""],"execution_count":null,"outputs":[]}]}